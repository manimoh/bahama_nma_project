{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Working_example_baseline_model.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOPuf5Vr/d3aUlQEqfxrGY/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/manimoh/bahama_nma_project/blob/master/Working_example_baseline_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wfCok0rGLXis",
        "colab_type": "code",
        "colab": {},
        "cellView": "form"
      },
      "source": [
        "#@title Data retrieval\n",
        "import os, requests\n",
        "\n",
        "fname = []\n",
        "for j in range(3):\n",
        "  fname.append('steinmetz_part%d.npz'%j)\n",
        "url = [\"https://osf.io/agvxh/download\"]\n",
        "url.append(\"https://osf.io/uv3mw/download\")\n",
        "url.append(\"https://osf.io/ehmw2/download\")\n",
        "\n",
        "for j in range(len(url)):\n",
        "  if not os.path.isfile(fname[j]):\n",
        "    try:\n",
        "      r = requests.get(url[j])\n",
        "    except requests.ConnectionError:\n",
        "      print(\"!!! Failed to download data !!!\")\n",
        "    else:\n",
        "      if r.status_code != requests.codes.ok:\n",
        "        print(\"!!! Failed to download data !!!\")\n",
        "      else:\n",
        "        with open(fname[j], \"wb\") as fid:\n",
        "          fid.write(r.content)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W6NzyMZkLbBd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "cellView": "form",
        "outputId": "725982d6-be37-45d2-8cdc-1e27914c5c65"
      },
      "source": [
        "#@title Data loading\n",
        "import numpy as np\n",
        "import copy\n",
        "\n",
        "alldat = np.array([])\n",
        "for j in range(len(fname)):\n",
        "    alldat = np.hstack((alldat, np.load('steinmetz_part%d.npz'%j, allow_pickle=True)['dat']))\n",
        "\n",
        "# select just one of the recordings here. 11 is nice because it has some neurons in vis ctx. \n",
        "dat = alldat[11]\n",
        "print(dat.keys())\n",
        "\n",
        "# Making a backup so that we don't have to load the data back up again\n",
        "alldat_backup = copy.copy(alldat)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "dict_keys(['spks', 'wheel', 'pupil', 'response', 'response_time', 'bin_size', 'stim_onset', 'contrast_right', 'contrast_left', 'brain_area', 'feedback_time', 'feedback_type', 'gocue', 'mouse_name', 'date_exp', 'trough_to_peak', 'active_trials', 'contrast_left_passive', 'contrast_right_passive', 'spks_passive', 'pupil_passive', 'wheel_passive', 'prev_reward', 'ccf', 'ccf_axes', 'cellid_orig', 'reaction_time', 'face', 'face_passive', 'licks', 'licks_passive'])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "azv1pTcrLfEi",
        "colab_type": "code",
        "colab": {},
        "cellView": "form"
      },
      "source": [
        "#@title Import statements (Add more as needed)\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "import random\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import KFold"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LIfglQEQLiLd",
        "colab_type": "code",
        "colab": {},
        "cellView": "form"
      },
      "source": [
        "#@title Helper functions\n",
        "def rebin_st(st, bin_size):\n",
        "    '''\n",
        "    Method to rebin a 2D spike train by summing the spikes from nearby bins\n",
        "\n",
        "    Inputs:\n",
        "    st: original spike_train of n neurons * t timebins\n",
        "    bin_size: Desired bin size (Must be a factor of 't')\n",
        "\n",
        "    Returns:\n",
        "    rst: spike_train of n neurons * bin_size timebins\n",
        "    '''\n",
        "    rst = np.zeros((st.shape[0], st.shape[1]//bin_size))\n",
        "    for i in range(rst.shape[1]):\n",
        "        rst[:,i] = np.sum(st[:,i*bin_size:(i+1)*bin_size], axis=1)\n",
        "    return rst\n",
        "\n",
        "\n",
        "def make_features_for_this_region (base_features):\n",
        "    '''\n",
        "    Method to make rows of a design_matrix with intra region interaction terms\n",
        "\n",
        "    Inputs:\n",
        "    base_features:\n",
        "    Top n principal components of this region\n",
        "\n",
        "    Returns:\n",
        "    A 1D numpy array with all the base features and second order interaction\n",
        "    terms \n",
        "    '''\n",
        "    features = []\n",
        "    # First add the base terms\n",
        "    for f in (base_features):\n",
        "        features.append(f)\n",
        "    # Next add the interaction terms\n",
        "    for i in range(0,len(base_features)):\n",
        "        features.append(base_features[i]**2)\n",
        "        for j in range(i+1, len(base_features)):\n",
        "            features.append(base_features[i]*base_features[j])\n",
        "    return np.array(features)\n",
        "\n",
        "\n",
        "def make_features_for_these_regions (base_features_1, base_features_2):\n",
        "    '''\n",
        "    Method to create cross-interactions terms from two regions\n",
        "\n",
        "    Inputs:\n",
        "    base_features_1 :\n",
        "    Top n principal components of this region\n",
        "    base_features_2 :\n",
        "    Top n principal components of this region\n",
        "\n",
        "    Returns:\n",
        "    A 1D numpy array with all the inter-region pairwise interaction terms\n",
        "    '''\n",
        "    features = []\n",
        "    #interactive_terms        \n",
        "    for i in range(0,len(base_features_1)):\n",
        "        for j in range(0, len(base_features_2)):\n",
        "            features.append(base_features_1[i]*base_features_2[j])\n",
        "    return np.array(features)\n",
        "\n",
        "def make_primary_featues (top_n, data):\n",
        "    '''\n",
        "    Method to return a dictionary of primary features based on the top_n PCA\n",
        "    components to be selected from the data\n",
        "\n",
        "    Inputs:\n",
        "    top_n :\n",
        "    Top n principal components to be selected\n",
        "    data :\n",
        "    Dictionary with brain region as key and the corresponding data as value\n",
        "\n",
        "    Returns:\n",
        "    primary_features_dict\n",
        "   Dictionary with brain region as key and the top_n features as value\n",
        "    '''\n",
        "    primary_features_dict = {}\n",
        "    for key in data:\n",
        "        this_data = data[key]\n",
        "        pca = PCA(n_components=top_n)\n",
        "        pca.fit(this_key)\n",
        "        this_pca = pca.components_\n",
        "        this_features = (this_pca @ this_data.T).T\n",
        "        primaary_features_dict[key] = this_features\n",
        "    return primary_features_dict\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VjH9UaXvRMB6",
        "colab_type": "code",
        "colab": {},
        "cellView": "form"
      },
      "source": [
        "#@title Preprocessing Data\n",
        "# Combining areas based on our 'expertise' (LOL)\n",
        "combined_areas = {\n",
        "    'MoCo': ['MOs', 'MOp'],\n",
        "    'VisualAreas': ['VISa', 'VISam', 'VISI', 'VISp', 'VISpm', 'VISrl'],\n",
        "    'Hippocampus' : ['CA1', 'CA2', 'CA3', 'DG'],\n",
        "    'SomNuc' : ['VPL', 'VPM']\n",
        "}\n",
        "# print(combined_areas.keys())\n",
        "\n",
        "# Creating a new key called 'new area' in alldat to add new_labels\n",
        "for dat in alldat:\n",
        "    new_labels = []\n",
        "    old_labels = dat['brain_area']\n",
        "    for label in old_labels:\n",
        "        found = False\n",
        "        for key in combined_areas:\n",
        "            if label in combined_areas[key]:\n",
        "                new_labels.append(key)\n",
        "                found = True\n",
        "                break\n",
        "        # if not present in the combined_areas dict\n",
        "        if not found:\n",
        "            new_labels.append(label)\n",
        "    dat['new_area'] = np.array(new_labels)\n",
        "\n",
        "sessions = [np.unique(dat['new_area']) for dat in alldat]\n",
        "chosen_ones = ['ACA', 'Hippocampus', 'MRN', 'MoCo', 'PL', 'SUB', 'VisualAreas']\n",
        "overlap  = pd.DataFrame(index=chosen_ones, columns=chosen_ones)\n",
        "for ses in sessions:\n",
        "    for i in range(len(ses)-1):\n",
        "        for j in range(i+1,len(ses)):\n",
        "            if ses[i] in chosen_ones and ses[j] in chosen_ones:\n",
        "                if overlap.loc[ses[i],ses[j]] is np.NaN:\n",
        "                    overlap.loc[ses[i],ses[j]] = 1\n",
        "                else:\n",
        "                    overlap.loc[ses[i],ses[j]] += 1\n",
        "                if overlap.loc[ses[j],ses[i]] is np.NaN:\n",
        "                    overlap.loc[ses[j],ses[i]] = 1\n",
        "                else:\n",
        "                    overlap.loc[ses[j],ses[i]] += 1\n",
        "# print(overlap)\n",
        "\n",
        "chosen_ones = ['ACA','Hippocampus','MoCo','PL', 'VisualAreas']\n",
        "sessions = [np.unique(dat['new_area']) for dat in alldat]\n",
        "overlap  = pd.DataFrame(index=chosen_ones, columns=chosen_ones)\n",
        "for ses in sessions:\n",
        "    for i in range(len(ses)-1):\n",
        "        for j in range(i+1,len(ses)):\n",
        "            if ses[i] in chosen_ones and ses[j] in chosen_ones:\n",
        "                if overlap.loc[ses[i],ses[j]] is np.NaN:\n",
        "                    overlap.loc[ses[i],ses[j]] = 1\n",
        "                else:\n",
        "                    overlap.loc[ses[i],ses[j]] += 1\n",
        "                if overlap.loc[ses[j],ses[i]] is np.NaN:\n",
        "                    overlap.loc[ses[j],ses[i]] = 1\n",
        "                else:\n",
        "                    overlap.loc[ses[j],ses[i]] += 1\n",
        "# print(overlap)\n",
        "\n",
        "# Making sure that the regions with all the chosen areas don't have other regions that are present in all\n",
        "# Uncomment the code below for sanity check \n",
        "# chosen_ones = ['ACA','Hippocampus','MoCo','PL', 'VisualAreas']\n",
        "# test = []\n",
        "# for session in alldat:\n",
        "#     session_areas = session['new_area']\n",
        "#     if (set(np.intersect1d(chosen_ones,np.unique(session_areas))) == set(chosen_ones)):\n",
        "#         test.append(session)\n",
        "# test_areas = [np.unique(x['new_area']) for x in test]\n",
        "# all_areas = np.unique(np.concatenate((test_areas)))\n",
        "# t_overlap  = pd.DataFrame(index=all_areas, columns=all_areas)\n",
        "\n",
        "# for i in range(len(test_areas)):\n",
        "#     for j in range(len(test_areas[i])-1):\n",
        "#         for k in range(i+1, len(test_areas[i])):\n",
        "#             if t_overlap.loc[test_areas[i][j],test_areas[i][k]] is np.NaN:\n",
        "#                 t_overlap.loc[test_areas[i][j],test_areas[i][k]] = 1\n",
        "#             else:\n",
        "#                 t_overlap.loc[test_areas[i][j],test_areas[i][k]] += 1\n",
        "#             if t_overlap.loc[test_areas[i][j],test_areas[i][k]] is np.NaN:\n",
        "#                 t_overlap.loc[test_areas[i][j],test_areas[i][k]] = 1\n",
        "#             else:\n",
        "#                 t_overlap.loc[test_areas[i][j],test_areas[i][k]] += 1\n",
        "\n",
        "# Next we want to restrict the dataset to the sessions which have the 5 above areas, and only the neurons which belong to these areas\n",
        "chosen_ones = ['ACA','Hippocampus','MoCo','PL', 'VisualAreas']\n",
        "copy_as_is = ['bin_size', 'stim_onset','mouse_name','date_exp','wheel','pupil','response', \\\n",
        "              'response_time','contrast_right','contrast_left','feedback_time','feedback_type',\\\n",
        "              'gocue', 'licks','face', 'reaction_time', 'ccf_axes', 'prev_reward']\n",
        "not_needed = ['waveform_w','waveform_u','brain_area_lfp','trough_to_peak','active_trials', \\\n",
        "              'contrast_left_passive','contrast_right_passive','spks_passive','lfp_passive',\\\n",
        "              'pupil_passive','wheel_passive', 'licks_passive', 'face_passive', 'cellid_orig' ]\n",
        "restricted_dat = []\n",
        "for session in alldat:\n",
        "    session_areas = session['new_area']\n",
        "    # Only if all the chosen_areas are in this session\n",
        "    res_session = {}\n",
        "    if (set(np.intersect1d(chosen_ones,np.unique(session_areas))) == set(chosen_ones)):\n",
        "        valid = []\n",
        "        for idx in range(len(session['new_area'])):\n",
        "            if session['new_area'][idx] in chosen_ones:\n",
        "                valid.append(idx)\n",
        "        for key in session:\n",
        "            if key not in not_needed:\n",
        "                if key in copy_as_is:\n",
        "                    res_session[key] = session[key]\n",
        "                else:\n",
        "                    #debug\n",
        "                    if key == 'spks':\n",
        "                        res_session[key] = np.take(session[key],valid, axis=0)\n",
        "                    else:\n",
        "                        res_session[key] = np.take(session[key],valid)\n",
        "        restricted_dat.append(res_session)\n",
        "\n",
        "# print(\"\\n Now let's look at the  number of neurons in the original areas\\n\")\n",
        "for dat in restricted_dat:\n",
        "    this_dict = {}\n",
        "    for x in np.unique(dat['brain_area']):\n",
        "        this_dict[x] = 0\n",
        "    for x in dat['brain_area']:\n",
        "        this_dict[x] += 1\n",
        "    # print(this_dict)\n",
        "\n",
        "# We decided to separate Hippocampus back into CA1 and DG since they have enough\n",
        "# neurons on their own, and the MoCO is just MOs. The Visual areas stay combined\n",
        "\n",
        "for dat in restricted_dat:\n",
        "    old_labels = dat['new_area']\n",
        "    og_labels = dat['brain_area']\n",
        "    for i in range(len(old_labels)):\n",
        "        if old_labels[i] == 'Hippocampus' or old_labels[i] =='MoCo':\n",
        "            old_labels[i] = og_labels[i]\n",
        "    dat['new_area'] = old_labels\n",
        "\n",
        "for dat in restricted_dat:\n",
        "    this_dict = {}\n",
        "    for x in np.unique(dat['new_area']):\n",
        "        this_dict[x] = 0\n",
        "    for x in dat['new_area']:\n",
        "        this_dict[x] += 1\n",
        "    # print(this_dict)\n",
        "\n",
        "# Uncomment the code below for sanity check \n",
        "# for x in restricted_dat:\n",
        "#     print(len(x['new_area']))\n",
        "# for key in restricted_dat[0]:\n",
        "#     try:\n",
        "#         print(key, ' : ', restricted_dat[0][key].shape)\n",
        "#     except:\n",
        "#         print(key, restricted_dat[0][key])\n",
        "\n",
        "# Reshape spikes from neurons x trials x timebins to trials x neurons x timebins\n",
        "\n",
        "for dat in restricted_dat:\n",
        "    dat['re_spks'] = np.swapaxes(dat['spks'], 0,1)\n",
        "\n",
        "# Uncomment the code below for sanity check    \n",
        "# for x in restricted_dat:\n",
        "#     print(len(x['new_area']))\n",
        "# for key in restricted_dat[0]:\n",
        "#     try:\n",
        "#         print(key, ' : ', restricted_dat[0][key].shape)\n",
        "#     except:\n",
        "#         print(key, restricted_dat[0][key])\n",
        "\n",
        "# for dat in restricted_dat:\n",
        "#     print(dat['gocue'].min(), dat['gocue'].max())\n",
        "\n",
        "# We decided to use a time window of 400 msec before the go-cue onset\n",
        "for dat in restricted_dat:\n",
        "    sum_spikes = []\n",
        "    for trial_num in range(len(dat['gocue'])):\n",
        "        this_gocue = dat['gocue'][trial_num]\n",
        "        # timebins = np.arange(0,2.5,0.01) #Incorrect!\n",
        "        timebins = np.arange(-0.5,2,0.01)\n",
        "        idx = np.argmin(np.abs(timebins - this_gocue))\n",
        "        spk_count = np.sum(dat['re_spks'][trial_num][:,idx-40:idx], axis = 1)\n",
        "        sum_spikes.append(spk_count)\n",
        "    # print(np.array(sum_spikes).shape)\n",
        "    dat['sum_spikes'] = np.array(sum_spikes)\n",
        "\n",
        "# Uncomment the code below for sanity check \n",
        "# for x in restricted_dat:\n",
        "#     print(len(x['new_area']))\n",
        "# for key in restricted_dat[0]:\n",
        "#     try:\n",
        "#         print(key, ' : ', restricted_dat[0][key].shape)\n",
        "#     except:\n",
        "#         print(key, restricted_dat[0][key])"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "70-1BcrtMIOC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
