{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Working_example_baseline_model.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyO2dEVfEJb4PRQB8Dqi1xOy",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/manimoh/bahama_nma_project/blob/master/Working_example_baseline_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wfCok0rGLXis",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Data retrieval\n",
        "import os, requests\n",
        "\n",
        "fname = []\n",
        "for j in range(3):\n",
        "  fname.append('steinmetz_part%d.npz'%j)\n",
        "url = [\"https://osf.io/agvxh/download\"]\n",
        "url.append(\"https://osf.io/uv3mw/download\")\n",
        "url.append(\"https://osf.io/ehmw2/download\")\n",
        "\n",
        "for j in range(len(url)):\n",
        "  if not os.path.isfile(fname[j]):\n",
        "    try:\n",
        "      r = requests.get(url[j])\n",
        "    except requests.ConnectionError:\n",
        "      print(\"!!! Failed to download data !!!\")\n",
        "    else:\n",
        "      if r.status_code != requests.codes.ok:\n",
        "        print(\"!!! Failed to download data !!!\")\n",
        "      else:\n",
        "        with open(fname[j], \"wb\") as fid:\n",
        "          fid.write(r.content)"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W6NzyMZkLbBd",
        "colab_type": "code",
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "8c471cdd-ce93-434a-cc29-7245130c8d12"
      },
      "source": [
        "#@title Data loading\n",
        "import numpy as np\n",
        "import copy\n",
        "\n",
        "alldat = np.array([])\n",
        "for j in range(len(fname)):\n",
        "    alldat = np.hstack((alldat, np.load('steinmetz_part%d.npz'%j, allow_pickle=True)['dat']))\n",
        "\n",
        "# select just one of the recordings here. 11 is nice because it has some neurons in vis ctx. \n",
        "dat = alldat[11]\n",
        "print(dat.keys())\n",
        "\n",
        "# Making a backup so that we don't have to load the data back up again\n",
        "alldat_backup = copy.copy(alldat)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "dict_keys(['spks', 'wheel', 'pupil', 'response', 'response_time', 'bin_size', 'stim_onset', 'contrast_right', 'contrast_left', 'brain_area', 'feedback_time', 'feedback_type', 'gocue', 'mouse_name', 'date_exp', 'trough_to_peak', 'active_trials', 'contrast_left_passive', 'contrast_right_passive', 'spks_passive', 'pupil_passive', 'wheel_passive', 'prev_reward', 'ccf', 'ccf_axes', 'cellid_orig', 'reaction_time', 'face', 'face_passive', 'licks', 'licks_passive'])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "azv1pTcrLfEi",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Import statements (Add more as needed)\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "import random\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import KFold"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LIfglQEQLiLd",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Helper functions\n",
        "def rebin_st(st, bin_size):\n",
        "    '''\n",
        "    Method to rebin a 2D spike train by summing the spikes from nearby bins\n",
        "\n",
        "    Inputs:\n",
        "    st: original spike_train of n neurons * t timebins\n",
        "    bin_size: Desired bin size (Must be a factor of 't')\n",
        "\n",
        "    Returns:\n",
        "    rst: spike_train of n neurons * bin_size timebins\n",
        "    '''\n",
        "    rst = np.zeros((st.shape[0], st.shape[1]//bin_size))\n",
        "    for i in range(rst.shape[1]):\n",
        "        rst[:,i] = np.sum(st[:,i*bin_size:(i+1)*bin_size], axis=1)\n",
        "    return rst\n",
        "\n",
        "\n",
        "def make_features_for_this_region (base_features):\n",
        "    '''\n",
        "    Method to make rows of a design_matrix with intra region interaction terms\n",
        "\n",
        "    Inputs:\n",
        "    base_features:\n",
        "    Top n principal components of this region\n",
        "\n",
        "    Returns:\n",
        "    A 1D numpy array with all the base features and second order interaction\n",
        "    terms \n",
        "    '''\n",
        "    features = []\n",
        "    # First add the base terms\n",
        "    for f in (base_features):\n",
        "        features.append(f)\n",
        "    # Next add the interaction terms\n",
        "    for i in range(0,len(base_features)):\n",
        "        features.append(base_features[i]**2)\n",
        "        for j in range(i+1, len(base_features)):\n",
        "            features.append(base_features[i]*base_features[j])\n",
        "    return np.array(features)\n",
        "\n",
        "\n",
        "def make_features_for_these_regions (base_features_1, base_features_2):\n",
        "    '''\n",
        "    Method to create cross-interactions terms from two regions\n",
        "\n",
        "    Inputs:\n",
        "    base_features_1 :\n",
        "    Top n principal components of this region\n",
        "    base_features_2 :\n",
        "    Top n principal components of this region\n",
        "\n",
        "    Returns:\n",
        "    A 1D numpy array with all the inter-region pairwise interaction terms\n",
        "    '''\n",
        "    features = []\n",
        "    #interactive_terms        \n",
        "    for i in range(0,len(base_features_1)):\n",
        "        for j in range(0, len(base_features_2)):\n",
        "            features.append(base_features_1[i]*base_features_2[j])\n",
        "    return np.array(features)\n",
        "\n",
        "def get_primary_features (data, top_n=3):\n",
        "    '''\n",
        "    Method to return a dictionary of primary features based on the top_n PCA\n",
        "    components to be selected from the data\n",
        "\n",
        "    Inputs:\n",
        "    data :\n",
        "    Dictionary with brain region as key and the corresponding data as value\n",
        "    top_n :\n",
        "    Top n principal components to be selected\n",
        "    \n",
        "\n",
        "    Returns:\n",
        "    primary_features_dict\n",
        "    \n",
        "    Dictionary with brain region as key and the top_n features as value\n",
        "    '''\n",
        "    primary_features_dict = {}\n",
        "    for key in data:\n",
        "        this_data = data[key]\n",
        "        pca = PCA(n_components=top_n)\n",
        "        pca.fit(this_data)\n",
        "        this_pca = pca.components_\n",
        "        this_features = (this_pca @ this_data.T).T\n",
        "        primary_features_dict[key] = this_features\n",
        "    return primary_features_dict\n"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VjH9UaXvRMB6",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Preprocessing Data\n",
        "# Combining areas based on our 'expertise' (LOL)\n",
        "combined_areas = {\n",
        "    'MoCo': ['MOs', 'MOp'],\n",
        "    'VisualAreas': ['VISa', 'VISam', 'VISI', 'VISp', 'VISpm', 'VISrl'],\n",
        "    'Hippocampus' : ['CA1', 'CA2', 'CA3', 'DG'],\n",
        "    'SomNuc' : ['VPL', 'VPM']\n",
        "}\n",
        "# print(combined_areas.keys())\n",
        "\n",
        "# Creating a new key called 'new area' in alldat to add new_labels\n",
        "for dat in alldat:\n",
        "    new_labels = []\n",
        "    old_labels = dat['brain_area']\n",
        "    for label in old_labels:\n",
        "        found = False\n",
        "        for key in combined_areas:\n",
        "            if label in combined_areas[key]:\n",
        "                new_labels.append(key)\n",
        "                found = True\n",
        "                break\n",
        "        # if not present in the combined_areas dict\n",
        "        if not found:\n",
        "            new_labels.append(label)\n",
        "    dat['new_area'] = np.array(new_labels)\n",
        "\n",
        "sessions = [np.unique(dat['new_area']) for dat in alldat]\n",
        "chosen_ones = ['ACA', 'Hippocampus', 'MRN', 'MoCo', 'PL', 'SUB', 'VisualAreas']\n",
        "overlap  = pd.DataFrame(index=chosen_ones, columns=chosen_ones)\n",
        "for ses in sessions:\n",
        "    for i in range(len(ses)-1):\n",
        "        for j in range(i+1,len(ses)):\n",
        "            if ses[i] in chosen_ones and ses[j] in chosen_ones:\n",
        "                if overlap.loc[ses[i],ses[j]] is np.NaN:\n",
        "                    overlap.loc[ses[i],ses[j]] = 1\n",
        "                else:\n",
        "                    overlap.loc[ses[i],ses[j]] += 1\n",
        "                if overlap.loc[ses[j],ses[i]] is np.NaN:\n",
        "                    overlap.loc[ses[j],ses[i]] = 1\n",
        "                else:\n",
        "                    overlap.loc[ses[j],ses[i]] += 1\n",
        "# print(overlap)\n",
        "\n",
        "chosen_ones = ['ACA','Hippocampus','MoCo','PL', 'VisualAreas']\n",
        "sessions = [np.unique(dat['new_area']) for dat in alldat]\n",
        "overlap  = pd.DataFrame(index=chosen_ones, columns=chosen_ones)\n",
        "for ses in sessions:\n",
        "    for i in range(len(ses)-1):\n",
        "        for j in range(i+1,len(ses)):\n",
        "            if ses[i] in chosen_ones and ses[j] in chosen_ones:\n",
        "                if overlap.loc[ses[i],ses[j]] is np.NaN:\n",
        "                    overlap.loc[ses[i],ses[j]] = 1\n",
        "                else:\n",
        "                    overlap.loc[ses[i],ses[j]] += 1\n",
        "                if overlap.loc[ses[j],ses[i]] is np.NaN:\n",
        "                    overlap.loc[ses[j],ses[i]] = 1\n",
        "                else:\n",
        "                    overlap.loc[ses[j],ses[i]] += 1\n",
        "# print(overlap)\n",
        "\n",
        "# Making sure that the regions with all the chosen areas don't have other regions that are present in all\n",
        "# Uncomment the code below for sanity check \n",
        "# chosen_ones = ['ACA','Hippocampus','MoCo','PL', 'VisualAreas']\n",
        "# test = []\n",
        "# for session in alldat:\n",
        "#     session_areas = session['new_area']\n",
        "#     if (set(np.intersect1d(chosen_ones,np.unique(session_areas))) == set(chosen_ones)):\n",
        "#         test.append(session)\n",
        "# test_areas = [np.unique(x['new_area']) for x in test]\n",
        "# all_areas = np.unique(np.concatenate((test_areas)))\n",
        "# t_overlap  = pd.DataFrame(index=all_areas, columns=all_areas)\n",
        "\n",
        "# for i in range(len(test_areas)):\n",
        "#     for j in range(len(test_areas[i])-1):\n",
        "#         for k in range(i+1, len(test_areas[i])):\n",
        "#             if t_overlap.loc[test_areas[i][j],test_areas[i][k]] is np.NaN:\n",
        "#                 t_overlap.loc[test_areas[i][j],test_areas[i][k]] = 1\n",
        "#             else:\n",
        "#                 t_overlap.loc[test_areas[i][j],test_areas[i][k]] += 1\n",
        "#             if t_overlap.loc[test_areas[i][j],test_areas[i][k]] is np.NaN:\n",
        "#                 t_overlap.loc[test_areas[i][j],test_areas[i][k]] = 1\n",
        "#             else:\n",
        "#                 t_overlap.loc[test_areas[i][j],test_areas[i][k]] += 1\n",
        "\n",
        "# Next we want to restrict the dataset to the sessions which have the 5 above areas, and only the neurons which belong to these areas\n",
        "chosen_ones = ['ACA','Hippocampus','MoCo','PL', 'VisualAreas']\n",
        "copy_as_is = ['bin_size', 'stim_onset','mouse_name','date_exp','wheel','pupil','response', \\\n",
        "              'response_time','contrast_right','contrast_left','feedback_time','feedback_type',\\\n",
        "              'gocue', 'licks','face', 'reaction_time', 'ccf_axes', 'prev_reward']\n",
        "not_needed = ['waveform_w','waveform_u','brain_area_lfp','trough_to_peak','active_trials', \\\n",
        "              'contrast_left_passive','contrast_right_passive','spks_passive','lfp_passive',\\\n",
        "              'pupil_passive','wheel_passive', 'licks_passive', 'face_passive', 'cellid_orig' ]\n",
        "restricted_dat = []\n",
        "for session in alldat:\n",
        "    session_areas = session['new_area']\n",
        "    # Only if all the chosen_areas are in this session\n",
        "    res_session = {}\n",
        "    if (set(np.intersect1d(chosen_ones,np.unique(session_areas))) == set(chosen_ones)):\n",
        "        valid = []\n",
        "        for idx in range(len(session['new_area'])):\n",
        "            if session['new_area'][idx] in chosen_ones:\n",
        "                valid.append(idx)\n",
        "        for key in session:\n",
        "            if key not in not_needed:\n",
        "                if key in copy_as_is:\n",
        "                    res_session[key] = session[key]\n",
        "                else:\n",
        "                    #debug\n",
        "                    if key == 'spks':\n",
        "                        res_session[key] = np.take(session[key],valid, axis=0)\n",
        "                    else:\n",
        "                        res_session[key] = np.take(session[key],valid)\n",
        "        restricted_dat.append(res_session)\n",
        "\n",
        "# print(\"\\n Now let's look at the  number of neurons in the original areas\\n\")\n",
        "for dat in restricted_dat:\n",
        "    this_dict = {}\n",
        "    for x in np.unique(dat['brain_area']):\n",
        "        this_dict[x] = 0\n",
        "    for x in dat['brain_area']:\n",
        "        this_dict[x] += 1\n",
        "    # print(this_dict)\n",
        "\n",
        "# We decided to separate Hippocampus back into CA1 and DG since they have enough\n",
        "# neurons on their own, and the MoCO is just MOs. The Visual areas stay combined\n",
        "\n",
        "for dat in restricted_dat:\n",
        "    old_labels = dat['new_area']\n",
        "    og_labels = dat['brain_area']\n",
        "    for i in range(len(old_labels)):\n",
        "        if old_labels[i] == 'Hippocampus' or old_labels[i] =='MoCo':\n",
        "            old_labels[i] = og_labels[i]\n",
        "    dat['new_area'] = old_labels\n",
        "\n",
        "for dat in restricted_dat:\n",
        "    this_dict = {}\n",
        "    for x in np.unique(dat['new_area']):\n",
        "        this_dict[x] = 0\n",
        "    for x in dat['new_area']:\n",
        "        this_dict[x] += 1\n",
        "    # print(this_dict)\n",
        "\n",
        "# Uncomment the code below for sanity check \n",
        "# for x in restricted_dat:\n",
        "#     print(len(x['new_area']))\n",
        "# for key in restricted_dat[0]:\n",
        "#     try:\n",
        "#         print(key, ' : ', restricted_dat[0][key].shape)\n",
        "#     except:\n",
        "#         print(key, restricted_dat[0][key])\n",
        "\n",
        "# Reshape spikes from neurons x trials x timebins to trials x neurons x timebins\n",
        "\n",
        "for dat in restricted_dat:\n",
        "    dat['re_spks'] = np.swapaxes(dat['spks'], 0,1)\n",
        "\n",
        "# Uncomment the code below for sanity check    \n",
        "# for x in restricted_dat:\n",
        "#     print(len(x['new_area']))\n",
        "# for key in restricted_dat[0]:\n",
        "#     try:\n",
        "#         print(key, ' : ', restricted_dat[0][key].shape)\n",
        "#     except:\n",
        "#         print(key, restricted_dat[0][key])\n",
        "\n",
        "# for dat in restricted_dat:\n",
        "#     print(dat['gocue'].min(), dat['gocue'].max())\n",
        "\n",
        "# We decided to use a time window of 400 msec before the go-cue onset\n",
        "for dat in restricted_dat:\n",
        "    sum_spikes = []\n",
        "    for trial_num in range(len(dat['gocue'])):\n",
        "        this_gocue = dat['gocue'][trial_num]\n",
        "        # timebins = np.arange(0,2.5,0.01) #Incorrect!\n",
        "        timebins = np.arange(-0.5,2,0.01)\n",
        "        idx = np.argmin(np.abs(timebins - this_gocue))\n",
        "        spk_count = np.sum(dat['re_spks'][trial_num][:,idx-40:idx], axis = 1)\n",
        "        sum_spikes.append(spk_count)\n",
        "    # print(np.array(sum_spikes).shape)\n",
        "    dat['sum_spikes'] = np.array(sum_spikes)\n",
        "\n",
        "# Uncomment the code below for sanity check \n",
        "# for x in restricted_dat:\n",
        "#     print(len(x['new_area']))\n",
        "# for key in restricted_dat[0]:\n",
        "#     try:\n",
        "#         print(key, ' : ', restricted_dat[0][key].shape)\n",
        "#     except:\n",
        "#         print(key, restricted_dat[0][key])"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "70-1BcrtMIOC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create regionwise data dictionary\n",
        "dat = restricted_dat[0]\n",
        "# print(dat['sum_spikes'].shape)\n",
        "regionwise_data = {}\n",
        "chosen_ones = ['ACA','CA1', 'DG', 'MOs','PL', 'VisualAreas']\n",
        "for area in chosen_ones:\n",
        "    idx = np.where(dat['new_area'] == area)[0]\n",
        "    this_area_neurons = np.take(dat['sum_spikes'],idx, axis=1)\n",
        "    regionwise_data[area] = this_area_neurons\n"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nvyYYRqgWmYF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Code block for using cross validation\n",
        "# Nested cross-validation algo as mentioned in\n",
        "# https://weina.me/nested-cross-validation\n",
        "\n",
        "# Design Matrix\n",
        "# Format: 1 row (1 _trial) ---> [region1_features region2_features region3_features ..]\n",
        "# Format of region1_features: pc1 pc2 pc3 pc1*pc1 pc1*pc2 pc1*pc3 pc2*pc2 pc2*pc3 pc3*pc3\n",
        "\n",
        "# Outer and inner loop values chosen so as to maintain 70-15-15\n",
        "# training-valid-test proportion\n",
        "\n",
        "trial_count = regionwise_data['CA1'].shape[0]\n",
        "all_idx = np.arange(0, trial_count)\n",
        "kf = KFold(n_splits = 7, shuffle = True, random_state=2020)\n",
        "ht_dict = {}\n",
        "\n",
        "for i in range(7):\n",
        "    ht_dict[i] = []\n",
        "htid = 0\n",
        "reported_accuracies = []\n",
        "for trainval, test_idx in kf.split(all_idx):\n",
        "    X_test = {}\n",
        "    X_tv = {}\n",
        "    for key in regionwise_data:\n",
        "        X_test[key] = np.take(regionwise_data[key], test_idx, axis=0)\n",
        "        X_tv[key] = np.take(regionwise_data[key], trainval, axis=0)\n",
        "    Y_test = np.take(dat['response'], test_idx, axis = 0)\n",
        "    Y_tv = np.take(dat['response'], trainval, axis = 0)\n",
        "\n",
        "    # make dm_trainval\n",
        "    primary_features_tv = get_primary_features(X_tv)\n",
        "    dm_tv = []\n",
        "    for i in range(X_tv['CA1'].shape[0]):\n",
        "        this_row = []\n",
        "        for key in X_test:\n",
        "            this_row.extend(make_features_for_this_region(primary_features_tv[key][i]))\n",
        "        dm_tv.append(this_row)\n",
        "\n",
        "    # make dm_test\n",
        "    primary_features_test = get_primary_features(X_test)\n",
        "    dm_test = []\n",
        "    for i in range(X_test['CA1'].shape[0]):\n",
        "        this_row = []\n",
        "        for key in X_test:\n",
        "            this_row.extend(make_features_for_this_region(primary_features_test[key][i]))\n",
        "        dm_test.append(this_row)\n",
        "\n",
        "    lratio_range = np.arange(0,1.01,0.1)\n",
        "    c_range = np.logspace(-4,0, num = 5)\n",
        "    # lratio_range = np.arange(0,1.01,0.4)\n",
        "    # c_range = np.logspace(-4,4, num = 4)\n",
        "    hyper_table = pd.DataFrame(index=lratio_range, columns=c_range)\n",
        "    # initialize table with empty lists\n",
        "    for i in lratio_range:\n",
        "        for j in c_range:\n",
        "            hyper_table.loc[i,j] = []\n",
        "\n",
        "    lf = KFold(n_splits = 5, shuffle = True, random_state=2020)\n",
        "    for train_idx, valid_idx in lf.split(trainval):\n",
        "        this_train = np.take(trainval, train_idx)\n",
        "        this_valid = np.take(trainval, valid_idx)\n",
        "        X_valid, X_train = {}, {}\n",
        "        for key in regionwise_data:\n",
        "            X_train[key] = np.take(regionwise_data[key], this_train, axis=0)\n",
        "            X_valid[key] = np.take(regionwise_data[key], this_valid, axis=0)\n",
        "        Y_train = np.take(dat['response'], this_train, axis = 0)\n",
        "        Y_valid = np.take(dat['response'], this_valid, axis = 0)\n",
        "        \n",
        "        # make dm_valid\n",
        "        primary_features_valid = get_primary_features(X_valid)\n",
        "        dm_valid = []\n",
        "        for i in range(X_valid['CA1'].shape[0]):\n",
        "            this_row = []\n",
        "            for key in X_valid:\n",
        "                this_row.extend(make_features_for_this_region(primary_features_valid[key][i]))\n",
        "            dm_valid.append(this_row)\n",
        "            \n",
        "        # make dm_train\n",
        "        primary_features_train = get_primary_features(X_train)\n",
        "        dm_train = []\n",
        "        for i in range(X_train['CA1'].shape[0]):\n",
        "            this_row = []\n",
        "            for key in X_train:\n",
        "                this_row.extend(make_features_for_this_region(primary_features_train[key][i]))\n",
        "            dm_train.append(this_row)\n",
        "\n",
        "        for c1 in lratio_range:\n",
        "            for c2 in c_range:\n",
        "                this_model = LogisticRegression(penalty='elasticnet', l1_ratio=c1, C=c2, solver='saga', fit_intercept=True, multi_class='multinomial', max_iter=10000).fit(dm_train, Y_train)\n",
        "                this_predict = this_model.predict(dm_valid)\n",
        "                this_correct = np.where(this_predict==Y_valid)[0].shape[0]\n",
        "                hyper_table.loc[c1,c2].append(this_correct/Y_valid.shape[0])\n",
        "    \n",
        "    # Now the inner L-fold have finished running\n",
        "    for c1 in lratio_range:\n",
        "        for c2 in c_range:\n",
        "            hyper_table.loc[c1,c2] = np.mean(hyper_table.loc[c1,c2])\n",
        "\n",
        "    # Find the optimal value of hyper-parameters based on max mean accuracy\n",
        "    this_ht = hyper_table.to_numpy(dtype ='float32')\n",
        "    c1_opt, c2_opt = np.where(this_ht == this_ht.max())\n",
        "    c1_opt, c2_opt = lratio_range[c1_opt[0]], c_range[c2_opt[0]]\n",
        "\n",
        "    ht_dict[htid] = this_ht #Saving all the hyperparameter tables\n",
        "    htid += 1\n",
        "\n",
        "\n",
        "    # Now train a model on trainval, based on these hyper-parameters\n",
        "    this_best_model = LogisticRegression(penalty='elasticnet', l1_ratio=c1_opt, C=c2_opt, solver='saga', fit_intercept=True, multi_class='multinomial', max_iter=10000).fit(dm_tv, Y_tv)\n",
        "    this_best_predict = this_best_model.predict(dm_test)\n",
        "    this_best_correct = np.where(this_best_predict==Y_test)[0].shape[0]\n",
        "    reported_accuracies.append(this_best_correct/Y_test.shape[0])\n",
        "\n",
        "    # Sanity check to see if it works\n",
        "    # a = hyper_table.to_numpy(dtype='float32')\n",
        "    # fig, ax = plt.subplots()\n",
        "    # pos = ax.imshow(a, cmap='jet')\n",
        "    # ax.set_yticks(np.arange(lratio_range.shape[0]))\n",
        "    # ax.set_yticklabels(np.round(lratio_range,2))\n",
        "    # ax.set_xticks(np.arange(c_range.shape[0]))\n",
        "    # ax.set_xticklabels(np.round(c_range,2))\n",
        "    # fig.colorbar(pos)\n",
        "    # break # Just to see if it works"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CyKWPKA0YGd-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "3b748336-c2d0-441b-b7b5-d6d926a3e531"
      },
      "source": [
        "print(\"Your model's reported accuracy is: \", np.round(np.mean(reported_accuracies)*100,2), \"%\")"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Your model's reported accuracy is:  52.36 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "72Bk1dhtYJbc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "outputId": "98bed39a-403c-46b4-9053-3fbba8ba9b5e"
      },
      "source": [
        "# Viewing hyper-parameter dict\n",
        "a = ht_dict[0]\n",
        "fig, ax = plt.subplots()\n",
        "pos = ax.imshow(a, cmap='jet')\n",
        "ax.set_yticks(np.arange(lratio_range.shape[0]))\n",
        "ax.set_yticklabels(np.round(lratio_range,2))\n",
        "ax.set_xticks(np.arange(c_range.shape[0]))\n",
        "ax.set_xticklabels(np.round(c_range,2))\n",
        "fig.colorbar(pos)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.colorbar.Colorbar at 0x7ff4b87cf630>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}