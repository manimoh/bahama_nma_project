{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Working_example_baseline_model.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyO2dEVfEJb4PRQB8Dqi1xOy",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/manimoh/bahama_nma_project/blob/master/Working_example_baseline_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wfCok0rGLXis",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "source": [
        "#@title Data retrieval\n",
        "import os, requests\n",
        "\n",
        "fname = []\n",
        "for j in range(3):\n",
        "  fname.append('steinmetz_part%d.npz'%j)\n",
        "url = [\"https://osf.io/agvxh/download\"]\n",
        "url.append(\"https://osf.io/uv3mw/download\")\n",
        "url.append(\"https://osf.io/ehmw2/download\")\n",
        "\n",
        "for j in range(len(url)):\n",
        "  if not os.path.isfile(fname[j]):\n",
        "    try:\n",
        "      r = requests.get(url[j])\n",
        "    except requests.ConnectionError:\n",
        "      print(\"!!! Failed to download data !!!\")\n",
        "    else:\n",
        "      if r.status_code != requests.codes.ok:\n",
        "        print(\"!!! Failed to download data !!!\")\n",
        "      else:\n",
        "        with open(fname[j], \"wb\") as fid:\n",
        "          fid.write(r.content)"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W6NzyMZkLbBd",
        "colab_type": "code",
        "cellView": "both",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "8c471cdd-ce93-434a-cc29-7245130c8d12"
      },
      "source": [
        "#@title Data loading\n",
        "import numpy as np\n",
        "import copy\n",
        "\n",
        "alldat = np.array([])\n",
        "for j in range(len(fname)):\n",
        "    alldat = np.hstack((alldat, np.load('steinmetz_part%d.npz'%j, allow_pickle=True)['dat']))\n",
        "\n",
        "# select just one of the recordings here. 11 is nice because it has some neurons in vis ctx. \n",
        "dat = alldat[11]\n",
        "print(dat.keys())\n",
        "\n",
        "# Making a backup so that we don't have to load the data back up again\n",
        "alldat_backup = copy.copy(alldat)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "dict_keys(['spks', 'wheel', 'pupil', 'response', 'response_time', 'bin_size', 'stim_onset', 'contrast_right', 'contrast_left', 'brain_area', 'feedback_time', 'feedback_type', 'gocue', 'mouse_name', 'date_exp', 'trough_to_peak', 'active_trials', 'contrast_left_passive', 'contrast_right_passive', 'spks_passive', 'pupil_passive', 'wheel_passive', 'prev_reward', 'ccf', 'ccf_axes', 'cellid_orig', 'reaction_time', 'face', 'face_passive', 'licks', 'licks_passive'])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "azv1pTcrLfEi",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "source": [
        "#@title Import statements (Add more as needed)\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "import random\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import KFold"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LIfglQEQLiLd",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "source": [
        "#@title Helper functions\n",
        "def rebin_st(st, bin_size):\n",
        "    '''\n",
        "    Method to rebin a 2D spike train by summing the spikes from nearby bins\n",
        "\n",
        "    Inputs:\n",
        "    st: original spike_train of n neurons * t timebins\n",
        "    bin_size: Desired bin size (Must be a factor of 't')\n",
        "\n",
        "    Returns:\n",
        "    rst: spike_train of n neurons * bin_size timebins\n",
        "    '''\n",
        "    rst = np.zeros((st.shape[0], st.shape[1]//bin_size))\n",
        "    for i in range(rst.shape[1]):\n",
        "        rst[:,i] = np.sum(st[:,i*bin_size:(i+1)*bin_size], axis=1)\n",
        "    return rst\n",
        "\n",
        "\n",
        "def make_features_for_this_region (base_features):\n",
        "    '''\n",
        "    Method to make rows of a design_matrix with intra region interaction terms\n",
        "\n",
        "    Inputs:\n",
        "    base_features:\n",
        "    Top n principal components of this region\n",
        "\n",
        "    Returns:\n",
        "    A 1D numpy array with all the base features and second order interaction\n",
        "    terms \n",
        "    '''\n",
        "    features = []\n",
        "    # First add the base terms\n",
        "    for f in (base_features):\n",
        "        features.append(f)\n",
        "    # Next add the interaction terms\n",
        "    for i in range(0,len(base_features)):\n",
        "        features.append(base_features[i]**2)\n",
        "        for j in range(i+1, len(base_features)):\n",
        "            features.append(base_features[i]*base_features[j])\n",
        "    return np.array(features)\n",
        "\n",
        "\n",
        "def make_features_for_these_regions (base_features_1, base_features_2):\n",
        "    '''\n",
        "    Method to create cross-interactions terms from two regions\n",
        "\n",
        "    Inputs:\n",
        "    base_features_1 :\n",
        "    Top n principal components of this region\n",
        "    base_features_2 :\n",
        "    Top n principal components of this region\n",
        "\n",
        "    Returns:\n",
        "    A 1D numpy array with all the inter-region pairwise interaction terms\n",
        "    '''\n",
        "    features = []\n",
        "    #interactive_terms        \n",
        "    for i in range(0,len(base_features_1)):\n",
        "        for j in range(0, len(base_features_2)):\n",
        "            features.append(base_features_1[i]*base_features_2[j])\n",
        "    return np.array(features)\n",
        "\n",
        "def get_primary_features (data, top_n=3):\n",
        "    '''\n",
        "    Method to return a dictionary of primary features based on the top_n PCA\n",
        "    components to be selected from the data\n",
        "\n",
        "    Inputs:\n",
        "    data :\n",
        "    Dictionary with brain region as key and the corresponding data as value\n",
        "    top_n :\n",
        "    Top n principal components to be selected\n",
        "    \n",
        "\n",
        "    Returns:\n",
        "    primary_features_dict\n",
        "    \n",
        "    Dictionary with brain region as key and the top_n features as value\n",
        "    '''\n",
        "    primary_features_dict = {}\n",
        "    for key in data:\n",
        "        this_data = data[key]\n",
        "        pca = PCA(n_components=top_n)\n",
        "        pca.fit(this_data)\n",
        "        this_pca = pca.components_\n",
        "        this_features = (this_pca @ this_data.T).T\n",
        "        primary_features_dict[key] = this_features\n",
        "    return primary_features_dict\n"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VjH9UaXvRMB6",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Preprocessing Data\n",
        "# Combining areas based on our 'expertise' (LOL)\n",
        "combined_areas = {\n",
        "    'MoCo': ['MOs', 'MOp'],\n",
        "    'VisualAreas': ['VISa', 'VISam', 'VISI', 'VISp', 'VISpm', 'VISrl'],\n",
        "    'Hippocampus' : ['CA1', 'CA2', 'CA3', 'DG'],\n",
        "    'SomNuc' : ['VPL', 'VPM']\n",
        "}\n",
        "# print(combined_areas.keys())\n",
        "\n",
        "# Creating a new key called 'new area' in alldat to add new_labels\n",
        "for dat in alldat:\n",
        "    new_labels = []\n",
        "    old_labels = dat['brain_area']\n",
        "    for label in old_labels:\n",
        "        found = False\n",
        "        for key in combined_areas:\n",
        "            if label in combined_areas[key]:\n",
        "                new_labels.append(key)\n",
        "                found = True\n",
        "                break\n",
        "        # if not present in the combined_areas dict\n",
        "        if not found:\n",
        "            new_labels.append(label)\n",
        "    dat['new_area'] = np.array(new_labels)\n",
        "\n",
        "sessions = [np.unique(dat['new_area']) for dat in alldat]\n",
        "chosen_ones = ['ACA', 'Hippocampus', 'MRN', 'MoCo', 'PL', 'SUB', 'VisualAreas']\n",
        "overlap  = pd.DataFrame(index=chosen_ones, columns=chosen_ones)\n",
        "for ses in sessions:\n",
        "    for i in range(len(ses)-1):\n",
        "        for j in range(i+1,len(ses)):\n",
        "            if ses[i] in chosen_ones and ses[j] in chosen_ones:\n",
        "                if overlap.loc[ses[i],ses[j]] is np.NaN:\n",
        "                    overlap.loc[ses[i],ses[j]] = 1\n",
        "                else:\n",
        "                    overlap.loc[ses[i],ses[j]] += 1\n",
        "                if overlap.loc[ses[j],ses[i]] is np.NaN:\n",
        "                    overlap.loc[ses[j],ses[i]] = 1\n",
        "                else:\n",
        "                    overlap.loc[ses[j],ses[i]] += 1\n",
        "# print(overlap)\n",
        "\n",
        "chosen_ones = ['ACA','Hippocampus','MoCo','PL', 'VisualAreas']\n",
        "sessions = [np.unique(dat['new_area']) for dat in alldat]\n",
        "overlap  = pd.DataFrame(index=chosen_ones, columns=chosen_ones)\n",
        "for ses in sessions:\n",
        "    for i in range(len(ses)-1):\n",
        "        for j in range(i+1,len(ses)):\n",
        "            if ses[i] in chosen_ones and ses[j] in chosen_ones:\n",
        "                if overlap.loc[ses[i],ses[j]] is np.NaN:\n",
        "                    overlap.loc[ses[i],ses[j]] = 1\n",
        "                else:\n",
        "                    overlap.loc[ses[i],ses[j]] += 1\n",
        "                if overlap.loc[ses[j],ses[i]] is np.NaN:\n",
        "                    overlap.loc[ses[j],ses[i]] = 1\n",
        "                else:\n",
        "                    overlap.loc[ses[j],ses[i]] += 1\n",
        "# print(overlap)\n",
        "\n",
        "# Making sure that the regions with all the chosen areas don't have other regions that are present in all\n",
        "# Uncomment the code below for sanity check \n",
        "# chosen_ones = ['ACA','Hippocampus','MoCo','PL', 'VisualAreas']\n",
        "# test = []\n",
        "# for session in alldat:\n",
        "#     session_areas = session['new_area']\n",
        "#     if (set(np.intersect1d(chosen_ones,np.unique(session_areas))) == set(chosen_ones)):\n",
        "#         test.append(session)\n",
        "# test_areas = [np.unique(x['new_area']) for x in test]\n",
        "# all_areas = np.unique(np.concatenate((test_areas)))\n",
        "# t_overlap  = pd.DataFrame(index=all_areas, columns=all_areas)\n",
        "\n",
        "# for i in range(len(test_areas)):\n",
        "#     for j in range(len(test_areas[i])-1):\n",
        "#         for k in range(i+1, len(test_areas[i])):\n",
        "#             if t_overlap.loc[test_areas[i][j],test_areas[i][k]] is np.NaN:\n",
        "#                 t_overlap.loc[test_areas[i][j],test_areas[i][k]] = 1\n",
        "#             else:\n",
        "#                 t_overlap.loc[test_areas[i][j],test_areas[i][k]] += 1\n",
        "#             if t_overlap.loc[test_areas[i][j],test_areas[i][k]] is np.NaN:\n",
        "#                 t_overlap.loc[test_areas[i][j],test_areas[i][k]] = 1\n",
        "#             else:\n",
        "#                 t_overlap.loc[test_areas[i][j],test_areas[i][k]] += 1\n",
        "\n",
        "# Next we want to restrict the dataset to the sessions which have the 5 above areas, and only the neurons which belong to these areas\n",
        "chosen_ones = ['ACA','Hippocampus','MoCo','PL', 'VisualAreas']\n",
        "copy_as_is = ['bin_size', 'stim_onset','mouse_name','date_exp','wheel','pupil','response', \\\n",
        "              'response_time','contrast_right','contrast_left','feedback_time','feedback_type',\\\n",
        "              'gocue', 'licks','face', 'reaction_time', 'ccf_axes', 'prev_reward']\n",
        "not_needed = ['waveform_w','waveform_u','brain_area_lfp','trough_to_peak','active_trials', \\\n",
        "              'contrast_left_passive','contrast_right_passive','spks_passive','lfp_passive',\\\n",
        "              'pupil_passive','wheel_passive', 'licks_passive', 'face_passive', 'cellid_orig' ]\n",
        "restricted_dat = []\n",
        "for session in alldat:\n",
        "    session_areas = session['new_area']\n",
        "    # Only if all the chosen_areas are in this session\n",
        "    res_session = {}\n",
        "    if (set(np.intersect1d(chosen_ones,np.unique(session_areas))) == set(chosen_ones)):\n",
        "        valid = []\n",
        "        for idx in range(len(session['new_area'])):\n",
        "            if session['new_area'][idx] in chosen_ones:\n",
        "                valid.append(idx)\n",
        "        for key in session:\n",
        "            if key not in not_needed:\n",
        "                if key in copy_as_is:\n",
        "                    res_session[key] = session[key]\n",
        "                else:\n",
        "                    #debug\n",
        "                    if key == 'spks':\n",
        "                        res_session[key] = np.take(session[key],valid, axis=0)\n",
        "                    else:\n",
        "                        res_session[key] = np.take(session[key],valid)\n",
        "        restricted_dat.append(res_session)\n",
        "\n",
        "# print(\"\\n Now let's look at the  number of neurons in the original areas\\n\")\n",
        "for dat in restricted_dat:\n",
        "    this_dict = {}\n",
        "    for x in np.unique(dat['brain_area']):\n",
        "        this_dict[x] = 0\n",
        "    for x in dat['brain_area']:\n",
        "        this_dict[x] += 1\n",
        "    # print(this_dict)\n",
        "\n",
        "# We decided to separate Hippocampus back into CA1 and DG since they have enough\n",
        "# neurons on their own, and the MoCO is just MOs. The Visual areas stay combined\n",
        "\n",
        "for dat in restricted_dat:\n",
        "    old_labels = dat['new_area']\n",
        "    og_labels = dat['brain_area']\n",
        "    for i in range(len(old_labels)):\n",
        "        if old_labels[i] == 'Hippocampus' or old_labels[i] =='MoCo':\n",
        "            old_labels[i] = og_labels[i]\n",
        "    dat['new_area'] = old_labels\n",
        "\n",
        "for dat in restricted_dat:\n",
        "    this_dict = {}\n",
        "    for x in np.unique(dat['new_area']):\n",
        "        this_dict[x] = 0\n",
        "    for x in dat['new_area']:\n",
        "        this_dict[x] += 1\n",
        "    # print(this_dict)\n",
        "\n",
        "# Uncomment the code below for sanity check \n",
        "# for x in restricted_dat:\n",
        "#     print(len(x['new_area']))\n",
        "# for key in restricted_dat[0]:\n",
        "#     try:\n",
        "#         print(key, ' : ', restricted_dat[0][key].shape)\n",
        "#     except:\n",
        "#         print(key, restricted_dat[0][key])\n",
        "\n",
        "# Reshape spikes from neurons x trials x timebins to trials x neurons x timebins\n",
        "\n",
        "for dat in restricted_dat:\n",
        "    dat['re_spks'] = np.swapaxes(dat['spks'], 0,1)\n",
        "\n",
        "# Uncomment the code below for sanity check    \n",
        "# for x in restricted_dat:\n",
        "#     print(len(x['new_area']))\n",
        "# for key in restricted_dat[0]:\n",
        "#     try:\n",
        "#         print(key, ' : ', restricted_dat[0][key].shape)\n",
        "#     except:\n",
        "#         print(key, restricted_dat[0][key])\n",
        "\n",
        "# for dat in restricted_dat:\n",
        "#     print(dat['gocue'].min(), dat['gocue'].max())\n",
        "\n",
        "# We decided to use a time window of 400 msec before the go-cue onset\n",
        "for dat in restricted_dat:\n",
        "    sum_spikes = []\n",
        "    for trial_num in range(len(dat['gocue'])):\n",
        "        this_gocue = dat['gocue'][trial_num]\n",
        "        # timebins = np.arange(0,2.5,0.01) #Incorrect!\n",
        "        timebins = np.arange(-0.5,2,0.01)\n",
        "        idx = np.argmin(np.abs(timebins - this_gocue))\n",
        "        spk_count = np.sum(dat['re_spks'][trial_num][:,idx-40:idx], axis = 1)\n",
        "        sum_spikes.append(spk_count)\n",
        "    # print(np.array(sum_spikes).shape)\n",
        "    dat['sum_spikes'] = np.array(sum_spikes)\n",
        "\n",
        "# Uncomment the code below for sanity check \n",
        "# for x in restricted_dat:\n",
        "#     print(len(x['new_area']))\n",
        "# for key in restricted_dat[0]:\n",
        "#     try:\n",
        "#         print(key, ' : ', restricted_dat[0][key].shape)\n",
        "#     except:\n",
        "#         print(key, restricted_dat[0][key])"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "70-1BcrtMIOC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create regionwise data dictionary\n",
        "dat = restricted_dat[0]\n",
        "# print(dat['sum_spikes'].shape)\n",
        "regionwise_data = {}\n",
        "chosen_ones = ['ACA','CA1', 'DG', 'MOs','PL', 'VisualAreas']\n",
        "for area in chosen_ones:\n",
        "    idx = np.where(dat['new_area'] == area)[0]\n",
        "    this_area_neurons = np.take(dat['sum_spikes'],idx, axis=1)\n",
        "    regionwise_data[area] = this_area_neurons\n"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nvyYYRqgWmYF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Code block for using cross validation\n",
        "# Nested cross-validation algo as mentioned in\n",
        "# https://weina.me/nested-cross-validation\n",
        "\n",
        "# Design Matrix\n",
        "# Format: 1 row (1 _trial) ---> [region1_features region2_features region3_features ..]\n",
        "# Format of region1_features: pc1 pc2 pc3 pc1*pc1 pc1*pc2 pc1*pc3 pc2*pc2 pc2*pc3 pc3*pc3\n",
        "\n",
        "# Outer and inner loop values chosen so as to maintain 70-15-15\n",
        "# training-valid-test proportion\n",
        "\n",
        "trial_count = regionwise_data['CA1'].shape[0]\n",
        "all_idx = np.arange(0, trial_count)\n",
        "kf = KFold(n_splits = 7, shuffle = True, random_state=2020)\n",
        "ht_dict = {}\n",
        "\n",
        "for i in range(7):\n",
        "    ht_dict[i] = []\n",
        "htid = 0\n",
        "reported_accuracies = []\n",
        "for trainval, test_idx in kf.split(all_idx):\n",
        "    X_test = {}\n",
        "    X_tv = {}\n",
        "    for key in regionwise_data:\n",
        "        X_test[key] = np.take(regionwise_data[key], test_idx, axis=0)\n",
        "        X_tv[key] = np.take(regionwise_data[key], trainval, axis=0)\n",
        "    Y_test = np.take(dat['response'], test_idx, axis = 0)\n",
        "    Y_tv = np.take(dat['response'], trainval, axis = 0)\n",
        "\n",
        "    # make dm_trainval\n",
        "    primary_features_tv = get_primary_features(X_tv)\n",
        "    dm_tv = []\n",
        "    for i in range(X_tv['CA1'].shape[0]):\n",
        "        this_row = []\n",
        "        for key in X_test:\n",
        "            this_row.extend(make_features_for_this_region(primary_features_tv[key][i]))\n",
        "        dm_tv.append(this_row)\n",
        "\n",
        "    # make dm_test\n",
        "    primary_features_test = get_primary_features(X_test)\n",
        "    dm_test = []\n",
        "    for i in range(X_test['CA1'].shape[0]):\n",
        "        this_row = []\n",
        "        for key in X_test:\n",
        "            this_row.extend(make_features_for_this_region(primary_features_test[key][i]))\n",
        "        dm_test.append(this_row)\n",
        "\n",
        "    lratio_range = np.arange(0,1.01,0.1)\n",
        "    c_range = np.logspace(-4,0, num = 5)\n",
        "    # lratio_range = np.arange(0,1.01,0.4)\n",
        "    # c_range = np.logspace(-4,4, num = 4)\n",
        "    hyper_table = pd.DataFrame(index=lratio_range, columns=c_range)\n",
        "    # initialize table with empty lists\n",
        "    for i in lratio_range:\n",
        "        for j in c_range:\n",
        "            hyper_table.loc[i,j] = []\n",
        "\n",
        "    lf = KFold(n_splits = 5, shuffle = True, random_state=2020)\n",
        "    for train_idx, valid_idx in lf.split(trainval):\n",
        "        this_train = np.take(trainval, train_idx)\n",
        "        this_valid = np.take(trainval, valid_idx)\n",
        "        X_valid, X_train = {}, {}\n",
        "        for key in regionwise_data:\n",
        "            X_train[key] = np.take(regionwise_data[key], this_train, axis=0)\n",
        "            X_valid[key] = np.take(regionwise_data[key], this_valid, axis=0)\n",
        "        Y_train = np.take(dat['response'], this_train, axis = 0)\n",
        "        Y_valid = np.take(dat['response'], this_valid, axis = 0)\n",
        "        \n",
        "        # make dm_valid\n",
        "        primary_features_valid = get_primary_features(X_valid)\n",
        "        dm_valid = []\n",
        "        for i in range(X_valid['CA1'].shape[0]):\n",
        "            this_row = []\n",
        "            for key in X_valid:\n",
        "                this_row.extend(make_features_for_this_region(primary_features_valid[key][i]))\n",
        "            dm_valid.append(this_row)\n",
        "            \n",
        "        # make dm_train\n",
        "        primary_features_train = get_primary_features(X_train)\n",
        "        dm_train = []\n",
        "        for i in range(X_train['CA1'].shape[0]):\n",
        "            this_row = []\n",
        "            for key in X_train:\n",
        "                this_row.extend(make_features_for_this_region(primary_features_train[key][i]))\n",
        "            dm_train.append(this_row)\n",
        "\n",
        "        for c1 in lratio_range:\n",
        "            for c2 in c_range:\n",
        "                this_model = LogisticRegression(penalty='elasticnet', l1_ratio=c1, C=c2, solver='saga', fit_intercept=True, multi_class='multinomial', max_iter=10000).fit(dm_train, Y_train)\n",
        "                this_predict = this_model.predict(dm_valid)\n",
        "                this_correct = np.where(this_predict==Y_valid)[0].shape[0]\n",
        "                hyper_table.loc[c1,c2].append(this_correct/Y_valid.shape[0])\n",
        "    \n",
        "    # Now the inner L-fold have finished running\n",
        "    for c1 in lratio_range:\n",
        "        for c2 in c_range:\n",
        "            hyper_table.loc[c1,c2] = np.mean(hyper_table.loc[c1,c2])\n",
        "\n",
        "    # Find the optimal value of hyper-parameters based on max mean accuracy\n",
        "    this_ht = hyper_table.to_numpy(dtype ='float32')\n",
        "    c1_opt, c2_opt = np.where(this_ht == this_ht.max())\n",
        "    c1_opt, c2_opt = lratio_range[c1_opt[0]], c_range[c2_opt[0]]\n",
        "\n",
        "    ht_dict[htid] = this_ht #Saving all the hyperparameter tables\n",
        "    htid += 1\n",
        "\n",
        "\n",
        "    # Now train a model on trainval, based on these hyper-parameters\n",
        "    this_best_model = LogisticRegression(penalty='elasticnet', l1_ratio=c1_opt, C=c2_opt, solver='saga', fit_intercept=True, multi_class='multinomial', max_iter=10000).fit(dm_tv, Y_tv)\n",
        "    this_best_predict = this_best_model.predict(dm_test)\n",
        "    this_best_correct = np.where(this_best_predict==Y_test)[0].shape[0]\n",
        "    reported_accuracies.append(this_best_correct/Y_test.shape[0])\n",
        "\n",
        "    # Sanity check to see if it works\n",
        "    # a = hyper_table.to_numpy(dtype='float32')\n",
        "    # fig, ax = plt.subplots()\n",
        "    # pos = ax.imshow(a, cmap='jet')\n",
        "    # ax.set_yticks(np.arange(lratio_range.shape[0]))\n",
        "    # ax.set_yticklabels(np.round(lratio_range,2))\n",
        "    # ax.set_xticks(np.arange(c_range.shape[0]))\n",
        "    # ax.set_xticklabels(np.round(c_range,2))\n",
        "    # fig.colorbar(pos)\n",
        "    # break # Just to see if it works"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CyKWPKA0YGd-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 165
        },
        "outputId": "c14b4261-fcc4-4b07-b481-77c03ee1d8d6"
      },
      "source": [
        "print(\"Your model's reported accuracy is: \", np.round(np.mean(reported_accuracies)*100,2), \"%\")"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-fef334f1a46a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Your model's reported accuracy is: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreported_accuracies\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"%\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'reported_accuracies' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "72Bk1dhtYJbc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Viewing hyper-parameter dict\n",
        "a = ht_dict[0]\n",
        "fig, ax = plt.subplots()\n",
        "pos = ax.imshow(a, cmap='jet')\n",
        "ax.set_yticks(np.arange(lratio_range.shape[0]))\n",
        "ax.set_yticklabels(np.round(lratio_range,2))\n",
        "ax.set_xticks(np.arange(c_range.shape[0]))\n",
        "ax.set_xticklabels(np.round(c_range,2))\n",
        "fig.colorbar(pos)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}