{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Working_example_baseline_model.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNnD59pLGW2KIg0sqhp+0P1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/manimoh/bahama_nma_project/blob/master/Working_example_baseline_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wfCok0rGLXis",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Data retrieval\n",
        "import os, requests\n",
        "\n",
        "fname = []\n",
        "for j in range(3):\n",
        "  fname.append('steinmetz_part%d.npz'%j)\n",
        "url = [\"https://osf.io/agvxh/download\"]\n",
        "url.append(\"https://osf.io/uv3mw/download\")\n",
        "url.append(\"https://osf.io/ehmw2/download\")\n",
        "\n",
        "for j in range(len(url)):\n",
        "  if not os.path.isfile(fname[j]):\n",
        "    try:\n",
        "      r = requests.get(url[j])\n",
        "    except requests.ConnectionError:\n",
        "      print(\"!!! Failed to download data !!!\")\n",
        "    else:\n",
        "      if r.status_code != requests.codes.ok:\n",
        "        print(\"!!! Failed to download data !!!\")\n",
        "      else:\n",
        "        with open(fname[j], \"wb\") as fid:\n",
        "          fid.write(r.content)"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W6NzyMZkLbBd",
        "colab_type": "code",
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "742b07ee-aa7f-425f-9914-7703fd76dfb0"
      },
      "source": [
        "#@title Data loading\n",
        "import numpy as np\n",
        "import copy\n",
        "\n",
        "alldat = np.array([])\n",
        "for j in range(len(fname)):\n",
        "    alldat = np.hstack((alldat, np.load('steinmetz_part%d.npz'%j, allow_pickle=True)['dat']))\n",
        "\n",
        "# select just one of the recordings here. 11 is nice because it has some neurons in vis ctx. \n",
        "dat = alldat[11]\n",
        "print(dat.keys())\n",
        "\n",
        "# Making a backup so that we don't have to load the data back up again\n",
        "alldat_backup = copy.copy(alldat)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "dict_keys(['spks', 'wheel', 'pupil', 'response', 'response_time', 'bin_size', 'stim_onset', 'contrast_right', 'contrast_left', 'brain_area', 'feedback_time', 'feedback_type', 'gocue', 'mouse_name', 'date_exp', 'trough_to_peak', 'active_trials', 'contrast_left_passive', 'contrast_right_passive', 'spks_passive', 'pupil_passive', 'wheel_passive', 'prev_reward', 'ccf', 'ccf_axes', 'cellid_orig', 'reaction_time', 'face', 'face_passive', 'licks', 'licks_passive'])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "azv1pTcrLfEi",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "source": [
        "#@title Import statements (Add more as needed)\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "import random\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.metrics import log_loss"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LIfglQEQLiLd",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Helper functions\n",
        "def rebin_st(st, bin_size):\n",
        "    '''\n",
        "    Method to rebin a 2D spike train by summing the spikes from nearby bins\n",
        "\n",
        "    Inputs:\n",
        "    st: original spike_train of n neurons * t timebins\n",
        "    bin_size: Desired bin size (Must be a factor of 't')\n",
        "\n",
        "    Returns:\n",
        "    rst: spike_train of n neurons * bin_size timebins\n",
        "    '''\n",
        "    rst = np.zeros((st.shape[0], st.shape[1]//bin_size))\n",
        "    for i in range(rst.shape[1]):\n",
        "        rst[:,i] = np.sum(st[:,i*bin_size:(i+1)*bin_size], axis=1)\n",
        "    return rst\n",
        "\n",
        "\n",
        "def make_features_for_this_region (base_features):\n",
        "    '''\n",
        "    Method to make rows of a design_matrix with intra region interaction terms\n",
        "\n",
        "    Inputs:\n",
        "    base_features:\n",
        "    Top n principal components of this region\n",
        "\n",
        "    Returns:\n",
        "    A 1D numpy array with all the base features and second order interaction\n",
        "    terms \n",
        "    '''\n",
        "    features = []\n",
        "    # First add the base terms\n",
        "    for f in (base_features):\n",
        "        features.append(f)\n",
        "    # Next add the interaction terms\n",
        "    for i in range(0,len(base_features)):\n",
        "        features.append(base_features[i]**2)\n",
        "        for j in range(i+1, len(base_features)):\n",
        "            features.append(base_features[i]*base_features[j])\n",
        "    return np.array(features)\n",
        "\n",
        "\n",
        "def make_features_for_these_regions (base_features_1, base_features_2):\n",
        "    '''\n",
        "    Method to create cross-interactions terms from two regions\n",
        "\n",
        "    Inputs:\n",
        "    base_features_1 :\n",
        "    Top n principal components of this region\n",
        "    base_features_2 :\n",
        "    Top n principal components of this region\n",
        "\n",
        "    Returns:\n",
        "    A 1D numpy array with all the inter-region pairwise interaction terms\n",
        "    '''\n",
        "    features = []\n",
        "    #interactive_terms        \n",
        "    for i in range(0,len(base_features_1)):\n",
        "        for j in range(0, len(base_features_2)):\n",
        "            features.append(base_features_1[i]*base_features_2[j])\n",
        "    return np.array(features)\n",
        "\n",
        "def get_primary_features (data, top_n=3):\n",
        "    '''\n",
        "    Method to return a dictionary of primary features based on the top_n PCA\n",
        "    components to be selected from the data\n",
        "\n",
        "    Inputs:\n",
        "    data :\n",
        "    Dictionary with brain region as key and the corresponding data as value\n",
        "    top_n :\n",
        "    Top n principal components to be selected\n",
        "    \n",
        "\n",
        "    Returns:\n",
        "    primary_features_dict\n",
        "    \n",
        "    Dictionary with brain region as key and the top_n features as value\n",
        "    '''\n",
        "    primary_features_dict = {}\n",
        "    for key in data:\n",
        "        this_data = data[key]\n",
        "        pca = PCA(n_components=top_n)\n",
        "        pca.fit(this_data)\n",
        "        this_pca = pca.components_\n",
        "        this_features = (this_pca @ this_data.T).T\n",
        "        primary_features_dict[key] = this_features\n",
        "    return primary_features_dict\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VjH9UaXvRMB6",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Preprocessing Data\n",
        "# Combining areas based on our 'expertise' (LOL)\n",
        "combined_areas = {\n",
        "    'MoCo': ['MOs', 'MOp'],\n",
        "    'VisualAreas': ['VISa', 'VISam', 'VISI', 'VISp', 'VISpm', 'VISrl'],\n",
        "    'Hippocampus' : ['CA1', 'CA2', 'CA3', 'DG'],\n",
        "    'SomNuc' : ['VPL', 'VPM']\n",
        "}\n",
        "# print(combined_areas.keys())\n",
        "\n",
        "# Creating a new key called 'new area' in alldat to add new_labels\n",
        "for dat in alldat:\n",
        "    new_labels = []\n",
        "    old_labels = dat['brain_area']\n",
        "    for label in old_labels:\n",
        "        found = False\n",
        "        for key in combined_areas:\n",
        "            if label in combined_areas[key]:\n",
        "                new_labels.append(key)\n",
        "                found = True\n",
        "                break\n",
        "        # if not present in the combined_areas dict\n",
        "        if not found:\n",
        "            new_labels.append(label)\n",
        "    dat['new_area'] = np.array(new_labels)\n",
        "\n",
        "sessions = [np.unique(dat['new_area']) for dat in alldat]\n",
        "chosen_ones = ['ACA', 'Hippocampus', 'MRN', 'MoCo', 'PL', 'SUB', 'VisualAreas']\n",
        "overlap  = pd.DataFrame(index=chosen_ones, columns=chosen_ones)\n",
        "for ses in sessions:\n",
        "    for i in range(len(ses)-1):\n",
        "        for j in range(i+1,len(ses)):\n",
        "            if ses[i] in chosen_ones and ses[j] in chosen_ones:\n",
        "                if overlap.loc[ses[i],ses[j]] is np.NaN:\n",
        "                    overlap.loc[ses[i],ses[j]] = 1\n",
        "                else:\n",
        "                    overlap.loc[ses[i],ses[j]] += 1\n",
        "                if overlap.loc[ses[j],ses[i]] is np.NaN:\n",
        "                    overlap.loc[ses[j],ses[i]] = 1\n",
        "                else:\n",
        "                    overlap.loc[ses[j],ses[i]] += 1\n",
        "# print(overlap)\n",
        "\n",
        "chosen_ones = ['ACA','Hippocampus','MoCo','PL', 'VisualAreas']\n",
        "sessions = [np.unique(dat['new_area']) for dat in alldat]\n",
        "overlap  = pd.DataFrame(index=chosen_ones, columns=chosen_ones)\n",
        "for ses in sessions:\n",
        "    for i in range(len(ses)-1):\n",
        "        for j in range(i+1,len(ses)):\n",
        "            if ses[i] in chosen_ones and ses[j] in chosen_ones:\n",
        "                if overlap.loc[ses[i],ses[j]] is np.NaN:\n",
        "                    overlap.loc[ses[i],ses[j]] = 1\n",
        "                else:\n",
        "                    overlap.loc[ses[i],ses[j]] += 1\n",
        "                if overlap.loc[ses[j],ses[i]] is np.NaN:\n",
        "                    overlap.loc[ses[j],ses[i]] = 1\n",
        "                else:\n",
        "                    overlap.loc[ses[j],ses[i]] += 1\n",
        "# print(overlap)\n",
        "\n",
        "# Making sure that the regions with all the chosen areas don't have other regions that are present in all\n",
        "# Uncomment the code below for sanity check \n",
        "# chosen_ones = ['ACA','Hippocampus','MoCo','PL', 'VisualAreas']\n",
        "# test = []\n",
        "# for session in alldat:\n",
        "#     session_areas = session['new_area']\n",
        "#     if (set(np.intersect1d(chosen_ones,np.unique(session_areas))) == set(chosen_ones)):\n",
        "#         test.append(session)\n",
        "# test_areas = [np.unique(x['new_area']) for x in test]\n",
        "# all_areas = np.unique(np.concatenate((test_areas)))\n",
        "# t_overlap  = pd.DataFrame(index=all_areas, columns=all_areas)\n",
        "\n",
        "# for i in range(len(test_areas)):\n",
        "#     for j in range(len(test_areas[i])-1):\n",
        "#         for k in range(i+1, len(test_areas[i])):\n",
        "#             if t_overlap.loc[test_areas[i][j],test_areas[i][k]] is np.NaN:\n",
        "#                 t_overlap.loc[test_areas[i][j],test_areas[i][k]] = 1\n",
        "#             else:\n",
        "#                 t_overlap.loc[test_areas[i][j],test_areas[i][k]] += 1\n",
        "#             if t_overlap.loc[test_areas[i][j],test_areas[i][k]] is np.NaN:\n",
        "#                 t_overlap.loc[test_areas[i][j],test_areas[i][k]] = 1\n",
        "#             else:\n",
        "#                 t_overlap.loc[test_areas[i][j],test_areas[i][k]] += 1\n",
        "\n",
        "# Next we want to restrict the dataset to the sessions which have the 5 above areas, and only the neurons which belong to these areas\n",
        "chosen_ones = ['ACA','Hippocampus','MoCo','PL', 'VisualAreas']\n",
        "copy_as_is = ['bin_size', 'stim_onset','mouse_name','date_exp','wheel','pupil','response', \\\n",
        "              'response_time','contrast_right','contrast_left','feedback_time','feedback_type',\\\n",
        "              'gocue', 'licks','face', 'reaction_time', 'ccf_axes', 'prev_reward']\n",
        "not_needed = ['waveform_w','waveform_u','brain_area_lfp','trough_to_peak','active_trials', \\\n",
        "              'contrast_left_passive','contrast_right_passive','spks_passive','lfp_passive',\\\n",
        "              'pupil_passive','wheel_passive', 'licks_passive', 'face_passive', 'cellid_orig' ]\n",
        "restricted_dat = []\n",
        "for session in alldat:\n",
        "    session_areas = session['new_area']\n",
        "    # Only if all the chosen_areas are in this session\n",
        "    res_session = {}\n",
        "    if (set(np.intersect1d(chosen_ones,np.unique(session_areas))) == set(chosen_ones)):\n",
        "        valid = []\n",
        "        for idx in range(len(session['new_area'])):\n",
        "            if session['new_area'][idx] in chosen_ones:\n",
        "                valid.append(idx)\n",
        "        for key in session:\n",
        "            if key not in not_needed:\n",
        "                if key in copy_as_is:\n",
        "                    res_session[key] = session[key]\n",
        "                else:\n",
        "                    #debug\n",
        "                    if key == 'spks':\n",
        "                        res_session[key] = np.take(session[key],valid, axis=0)\n",
        "                    else:\n",
        "                        res_session[key] = np.take(session[key],valid)\n",
        "        restricted_dat.append(res_session)\n",
        "\n",
        "# print(\"\\n Now let's look at the  number of neurons in the original areas\\n\")\n",
        "for dat in restricted_dat:\n",
        "    this_dict = {}\n",
        "    for x in np.unique(dat['brain_area']):\n",
        "        this_dict[x] = 0\n",
        "    for x in dat['brain_area']:\n",
        "        this_dict[x] += 1\n",
        "    # print(this_dict)\n",
        "\n",
        "# We decided to separate Hippocampus back into CA1 and DG since they have enough\n",
        "# neurons on their own, and the MoCO is just MOs. The Visual areas stay combined\n",
        "\n",
        "for dat in restricted_dat:\n",
        "    old_labels = dat['new_area']\n",
        "    og_labels = dat['brain_area']\n",
        "    for i in range(len(old_labels)):\n",
        "        if old_labels[i] == 'Hippocampus' or old_labels[i] =='MoCo':\n",
        "            old_labels[i] = og_labels[i]\n",
        "    dat['new_area'] = old_labels\n",
        "\n",
        "for dat in restricted_dat:\n",
        "    this_dict = {}\n",
        "    for x in np.unique(dat['new_area']):\n",
        "        this_dict[x] = 0\n",
        "    for x in dat['new_area']:\n",
        "        this_dict[x] += 1\n",
        "    # print(this_dict)\n",
        "\n",
        "# Uncomment the code below for sanity check \n",
        "# for x in restricted_dat:\n",
        "#     print(len(x['new_area']))\n",
        "# for key in restricted_dat[0]:\n",
        "#     try:\n",
        "#         print(key, ' : ', restricted_dat[0][key].shape)\n",
        "#     except:\n",
        "#         print(key, restricted_dat[0][key])\n",
        "\n",
        "# Reshape spikes from neurons x trials x timebins to trials x neurons x timebins\n",
        "\n",
        "for dat in restricted_dat:\n",
        "    dat['re_spks'] = np.swapaxes(dat['spks'], 0,1)\n",
        "\n",
        "# Uncomment the code below for sanity check    \n",
        "# for x in restricted_dat:\n",
        "#     print(len(x['new_area']))\n",
        "# for key in restricted_dat[0]:\n",
        "#     try:\n",
        "#         print(key, ' : ', restricted_dat[0][key].shape)\n",
        "#     except:\n",
        "#         print(key, restricted_dat[0][key])\n",
        "\n",
        "# for dat in restricted_dat:\n",
        "#     print(dat['gocue'].min(), dat['gocue'].max())\n",
        "\n",
        "# We decided to use a time window of 400 msec before the go-cue onset\n",
        "for dat in restricted_dat:\n",
        "    sum_spikes = []\n",
        "    for trial_num in range(len(dat['gocue'])):\n",
        "        this_gocue = dat['gocue'][trial_num]\n",
        "        # timebins = np.arange(0,2.5,0.01) #Incorrect!\n",
        "        timebins = np.arange(-0.5,2,0.01)\n",
        "        idx = np.argmin(np.abs(timebins - this_gocue))\n",
        "        spk_count = np.sum(dat['re_spks'][trial_num][:,idx-40:idx], axis = 1)\n",
        "        sum_spikes.append(spk_count)\n",
        "    # print(np.array(sum_spikes).shape)\n",
        "    dat['sum_spikes'] = np.array(sum_spikes)\n",
        "\n",
        "# Uncomment the code below for sanity check \n",
        "# for x in restricted_dat:\n",
        "#     print(len(x['new_area']))\n",
        "# for key in restricted_dat[0]:\n",
        "#     try:\n",
        "#         print(key, ' : ', restricted_dat[0][key].shape)\n",
        "#     except:\n",
        "#         print(key, restricted_dat[0][key])"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "70-1BcrtMIOC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create regionwise data dictionary\n",
        "dat = restricted_dat[0]\n",
        "# print(dat['sum_spikes'].shape)\n",
        "regionwise_data = {}\n",
        "chosen_ones = ['ACA','CA1', 'DG', 'MOs','PL', 'VisualAreas']\n",
        "for area in chosen_ones:\n",
        "    idx = np.where(dat['new_area'] == area)[0]\n",
        "    this_area_neurons = np.take(dat['sum_spikes'],idx, axis=1)\n",
        "    regionwise_data[area] = this_area_neurons\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nvyYYRqgWmYF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "f1a59000-4ad4-4549-e30e-84d44c98d954"
      },
      "source": [
        "# Code block for using cross validation\n",
        "# Nested cross-validation algo as mentioned in\n",
        "# https://weina.me/nested-cross-validation\n",
        "\n",
        "# Design Matrix\n",
        "# Format: 1 row (1 _trial) ---> [region1_features region2_features region3_features ..]\n",
        "# Format of region1_features: pc1 pc2 pc3 pc1*pc1 pc1*pc2 pc1*pc3 pc2*pc2 pc2*pc3 pc3*pc3\n",
        "\n",
        "# Outer and inner loop values chosen so as to maintain 70-15-15\n",
        "# training-valid-test proportion\n",
        "\n",
        "trial_count = regionwise_data['CA1'].shape[0]\n",
        "all_idx = np.arange(0, trial_count)\n",
        "kf = KFold(n_splits = 7, shuffle = True, random_state=2020)\n",
        "ht_dict = {}\n",
        "\n",
        "for i in range(7):\n",
        "    ht_dict[i] = []\n",
        "htid = 0\n",
        "reported_accuracies = []\n",
        "reported_NLL = []\n",
        "for trainval, test_idx in kf.split(all_idx):\n",
        "    X_test = {}\n",
        "    X_tv = {}\n",
        "    for key in regionwise_data:\n",
        "        X_test[key] = np.take(regionwise_data[key], test_idx, axis=0)\n",
        "        X_tv[key] = np.take(regionwise_data[key], trainval, axis=0)\n",
        "    Y_test = np.take(dat['response'], test_idx, axis = 0)\n",
        "    Y_tv = np.take(dat['response'], trainval, axis = 0)\n",
        "\n",
        "    # make dm_trainval\n",
        "    primary_features_tv = get_primary_features(X_tv)\n",
        "    dm_tv = []\n",
        "    for i in range(X_tv['CA1'].shape[0]):\n",
        "        this_row = []\n",
        "        for key in X_test:\n",
        "            this_row.extend(make_features_for_this_region(primary_features_tv[key][i]))\n",
        "        dm_tv.append(this_row)\n",
        "\n",
        "    # make dm_test\n",
        "    primary_features_test = get_primary_features(X_test)\n",
        "    dm_test = []\n",
        "    for i in range(X_test['CA1'].shape[0]):\n",
        "        this_row = []\n",
        "        for key in X_test:\n",
        "            this_row.extend(make_features_for_this_region(primary_features_test[key][i]))\n",
        "        dm_test.append(this_row)\n",
        "\n",
        "    lratio_range = np.arange(0,1.01,0.1)\n",
        "    c_range = np.logspace(-4,0, num = 5)\n",
        "    # lratio_range = np.arange(0,1.01,0.4)\n",
        "    # c_range = np.logspace(-4,4, num = 4)\n",
        "    hyper_table = pd.DataFrame(index=lratio_range, columns=c_range)\n",
        "    # initialize table with empty lists\n",
        "    for i in lratio_range:\n",
        "        for j in c_range:\n",
        "            hyper_table.loc[i,j] = []\n",
        "\n",
        "    lf = KFold(n_splits = 5, shuffle = True, random_state=2020)\n",
        "    for train_idx, valid_idx in lf.split(trainval):\n",
        "        this_train = np.take(trainval, train_idx)\n",
        "        this_valid = np.take(trainval, valid_idx)\n",
        "        X_valid, X_train = {}, {}\n",
        "        for key in regionwise_data:\n",
        "            X_train[key] = np.take(regionwise_data[key], this_train, axis=0)\n",
        "            X_valid[key] = np.take(regionwise_data[key], this_valid, axis=0)\n",
        "        Y_train = np.take(dat['response'], this_train, axis = 0)\n",
        "        Y_valid = np.take(dat['response'], this_valid, axis = 0)\n",
        "        \n",
        "        # make dm_valid\n",
        "        primary_features_valid = get_primary_features(X_valid)\n",
        "        dm_valid = []\n",
        "        for i in range(X_valid['CA1'].shape[0]):\n",
        "            this_row = []\n",
        "            for key in X_valid:\n",
        "                this_row.extend(make_features_for_this_region(primary_features_valid[key][i]))\n",
        "            dm_valid.append(this_row)\n",
        "            \n",
        "        # make dm_train\n",
        "        primary_features_train = get_primary_features(X_train)\n",
        "        dm_train = []\n",
        "        for i in range(X_train['CA1'].shape[0]):\n",
        "            this_row = []\n",
        "            for key in X_train:\n",
        "                this_row.extend(make_features_for_this_region(primary_features_train[key][i]))\n",
        "            dm_train.append(this_row)\n",
        "\n",
        "        for c1 in lratio_range:\n",
        "            for c2 in c_range:\n",
        "                this_model = LogisticRegression(penalty='elasticnet', l1_ratio=c1, C=c2, solver='saga', fit_intercept=True, multi_class='multinomial', max_iter=10000).fit(dm_train, Y_train)\n",
        "                this_predict = this_model.predict_proba(dm_valid)\n",
        "                # this_correct = np.where(this_predict==Y_valid)[0].shape[0]\n",
        "                # hyper_table.loc[c1,c2].append(this_correct/Y_valid.shape[0])\n",
        "                this_nLL = log_loss(Y_valid, this_predict, labels=np.array([-1,0,1], dtype='float32'))\n",
        "                hyper_table.loc[c1,c2].append(this_nLL)\n",
        "    \n",
        "    # Now the inner L-fold have finished running\n",
        "    for c1 in lratio_range:\n",
        "        for c2 in c_range:\n",
        "            hyper_table.loc[c1,c2] = np.mean(hyper_table.loc[c1,c2])\n",
        "\n",
        "    # Find the optimal value of hyper-parameters based on minimum nLL\n",
        "    this_ht = hyper_table.to_numpy(dtype ='float32')\n",
        "    c1_opt, c2_opt = np.where(this_ht == this_ht.min())\n",
        "    c1_opt, c2_opt = lratio_range[c1_opt[0]], c_range[c2_opt[0]]\n",
        "\n",
        "    ht_dict[htid] = this_ht #Saving all the hyperparameter tables\n",
        "    htid += 1\n",
        "\n",
        "\n",
        "    # Now train a model on trainval, based on these hyper-parameters\n",
        "    this_best_model = LogisticRegression(penalty='elasticnet', l1_ratio=c1_opt, C=c2_opt, solver='saga', fit_intercept=True, multi_class='multinomial', max_iter=10000).fit(dm_tv, Y_tv)\n",
        "    this_best_prob = this_best_model.predict_proba(dm_test)\n",
        "    this_best_predict = this_best_model.predict(dm_test)\n",
        "    this_best_correct = np.where(this_best_predict==Y_test)[0].shape[0]\n",
        "    reported_accuracies.append(this_best_correct/Y_test.shape[0])\n",
        "    this_best_nLL = log_loss(Y_test, this_best_prob, labels=np.array([-1,0,1], dtype='float32'))\n",
        "    reported_NLL.append(this_best_nLL)\n",
        "\n",
        "    # Sanity check to see if it works\n",
        "    # a = hyper_table.to_numpy(dtype='float32')\n",
        "    # fig, ax = plt.subplots()\n",
        "    # pos = ax.imshow(a, cmap='jet')\n",
        "    # ax.set_yticks(np.arange(lratio_range.shape[0]))\n",
        "    # ax.set_yticklabels(np.round(lratio_range,2))\n",
        "    # ax.set_xticks(np.arange(c_range.shape[0]))\n",
        "    # ax.set_xticklabels(np.round(c_range,2))\n",
        "    # fig.colorbar(pos)\n",
        "    # break # Just to see if it works\n",
        "\n",
        "print(\"Your model's reported accuracy is: \", np.round(np.mean(reported_accuracies)*100,2), \"%\")\n",
        "print(\"Your model's reported nLL is: \", np.mean(reported_NLL))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Your model's reported accuracy is:  47.99 %\n",
            "Your model's reported nLL is:  1.0394885259014137\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "72Bk1dhtYJbc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "outputId": "e0732731-29c6-48c5-c69e-48504ddd0af6"
      },
      "source": [
        "# Viewing hyper-parameter dict\n",
        "a = ht_dict[0]\n",
        "fig, ax = plt.subplots()\n",
        "pos = ax.imshow(a, cmap='jet')\n",
        "ax.set_yticks(np.arange(lratio_range.shape[0]))\n",
        "ax.set_yticklabels(np.round(lratio_range,2))\n",
        "ax.set_xticks(np.arange(c_range.shape[0]))\n",
        "ax.set_xticklabels(np.round(c_range,2))\n",
        "fig.colorbar(pos)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.colorbar.Colorbar at 0x7f8f8e188550>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}