{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/manimoh/bahama_nma_project/blob/master/Working_example_baseline_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cellView": "form",
    "colab": {},
    "colab_type": "code",
    "id": "wfCok0rGLXis"
   },
   "outputs": [],
   "source": [
    "#@title Data retrieval\n",
    "import os, requests\n",
    "\n",
    "fname = []\n",
    "for j in range(3):\n",
    "  fname.append('steinmetz_part%d.npz'%j)\n",
    "url = [\"https://osf.io/agvxh/download\"]\n",
    "url.append(\"https://osf.io/uv3mw/download\")\n",
    "url.append(\"https://osf.io/ehmw2/download\")\n",
    "\n",
    "for j in range(len(url)):\n",
    "  if not os.path.isfile(fname[j]):\n",
    "    try:\n",
    "      r = requests.get(url[j])\n",
    "    except requests.ConnectionError:\n",
    "      print(\"!!! Failed to download data !!!\")\n",
    "    else:\n",
    "      if r.status_code != requests.codes.ok:\n",
    "        print(\"!!! Failed to download data !!!\")\n",
    "      else:\n",
    "        with open(fname[j], \"wb\") as fid:\n",
    "          fid.write(r.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "W6NzyMZkLbBd",
    "outputId": "742b07ee-aa7f-425f-9914-7703fd76dfb0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['spks', 'wheel', 'pupil', 'response', 'response_time', 'bin_size', 'stim_onset', 'contrast_right', 'contrast_left', 'brain_area', 'feedback_time', 'feedback_type', 'gocue', 'mouse_name', 'date_exp', 'trough_to_peak', 'active_trials', 'contrast_left_passive', 'contrast_right_passive', 'spks_passive', 'pupil_passive', 'wheel_passive', 'prev_reward', 'ccf', 'ccf_axes', 'cellid_orig', 'reaction_time', 'face', 'face_passive', 'licks', 'licks_passive'])\n"
     ]
    }
   ],
   "source": [
    "#@title Data loading\n",
    "import numpy as np\n",
    "import copy\n",
    "\n",
    "alldat = np.array([])\n",
    "for j in range(len(fname)):\n",
    "    alldat = np.hstack((alldat, np.load('steinmetz_part%d.npz'%j, allow_pickle=True)['dat']))\n",
    "\n",
    "# select just one of the recordings here. 11 is nice because it has some neurons in vis ctx. \n",
    "dat = alldat[11]\n",
    "print(dat.keys())\n",
    "\n",
    "# Making a backup so that we don't have to load the data back up again\n",
    "alldat_backup = copy.copy(alldat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "cellView": "both",
    "colab": {},
    "colab_type": "code",
    "id": "azv1pTcrLfEi"
   },
   "outputs": [],
   "source": [
    "#@title Import statements (Add more as needed)\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import log_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "cellView": "form",
    "colab": {},
    "colab_type": "code",
    "id": "LIfglQEQLiLd"
   },
   "outputs": [],
   "source": [
    "#@title Helper functions\n",
    "def rebin_st(st, bin_size):\n",
    "    '''\n",
    "    Method to rebin a 2D spike train by summing the spikes from nearby bins\n",
    "\n",
    "    Inputs:\n",
    "    st: original spike_train of n neurons * t timebins\n",
    "    bin_size: Desired bin size (Must be a factor of 't')\n",
    "\n",
    "    Returns:\n",
    "    rst: spike_train of n neurons * bin_size timebins\n",
    "    '''\n",
    "    rst = np.zeros((st.shape[0], st.shape[1]//bin_size))\n",
    "    for i in range(rst.shape[1]):\n",
    "        rst[:,i] = np.sum(st[:,i*bin_size:(i+1)*bin_size], axis=1)\n",
    "    return rst\n",
    "\n",
    "\n",
    "def make_features_for_this_region (base_features):\n",
    "    '''\n",
    "    Method to make rows of a design_matrix with intra region interaction terms\n",
    "\n",
    "    Inputs:\n",
    "    base_features:\n",
    "    Top n principal components of this region\n",
    "\n",
    "    Returns:\n",
    "    A 1D numpy array with all the base features and second order interaction\n",
    "    terms \n",
    "    '''\n",
    "    features = []\n",
    "    # First add the base terms\n",
    "    for f in (base_features):\n",
    "        features.append(f)\n",
    "    # Next add the interaction terms\n",
    "    for i in range(0,len(base_features)):\n",
    "        features.append(base_features[i]**2)\n",
    "        for j in range(i+1, len(base_features)):\n",
    "            features.append(base_features[i]*base_features[j])\n",
    "    return np.array(features)\n",
    "\n",
    "\n",
    "def make_features_for_these_regions (base_features_1, base_features_2):\n",
    "    '''\n",
    "    Method to create cross-interactions terms from two regions\n",
    "\n",
    "    Inputs:\n",
    "    base_features_1 :\n",
    "    Top n principal components of this region\n",
    "    base_features_2 :\n",
    "    Top n principal components of this region\n",
    "\n",
    "    Returns:\n",
    "    A 1D numpy array with all the inter-region pairwise interaction terms\n",
    "    '''\n",
    "    features = []\n",
    "    #interactive_terms        \n",
    "    for i in range(0,len(base_features_1)):\n",
    "        for j in range(0, len(base_features_2)):\n",
    "            features.append(base_features_1[i]*base_features_2[j])\n",
    "    return np.array(features)\n",
    "\n",
    "def get_primary_features (data, top_n=3):\n",
    "    '''\n",
    "    Method to return a dictionary of primary features based on the top_n PCA\n",
    "    components to be selected from the data\n",
    "\n",
    "    Inputs:\n",
    "    data :\n",
    "    Dictionary with brain region as key and the corresponding data as value\n",
    "    top_n :\n",
    "    Top n principal components to be selected\n",
    "    \n",
    "\n",
    "    Returns:\n",
    "    primary_features_dict\n",
    "    \n",
    "    Dictionary with brain region as key and the top_n features as value\n",
    "    '''\n",
    "    primary_features_dict = {}\n",
    "    for key in data:\n",
    "        this_data = data[key]\n",
    "        pca = PCA(n_components=top_n)\n",
    "        pca.fit(this_data)\n",
    "        this_pca = pca.components_\n",
    "        this_features = (this_pca @ this_data.T).T\n",
    "        primary_features_dict[key] = this_features\n",
    "    return primary_features_dict\n",
    "\n",
    "def normalize_dm (dm_in):\n",
    "    '''\n",
    "    Method to return the dm with columns normalized such that each column's standard deviation is 1\n",
    "    '''\n",
    "    dm_out = np.array(dm_in)\n",
    "    for col in range(dm_out.shape[1]):\n",
    "        norm = dm_out[:,col].std()\n",
    "        dm_out[:,col] =  dm_out[:,col]/norm\n",
    "    return dm_out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "cellView": "form",
    "colab": {},
    "colab_type": "code",
    "id": "VjH9UaXvRMB6"
   },
   "outputs": [],
   "source": [
    "#@title Preprocessing Data\n",
    "# Combining areas based on our 'expertise' (LOL)\n",
    "combined_areas = {\n",
    "    'MoCo': ['MOs', 'MOp'],\n",
    "    'VisualAreas': ['VISa', 'VISam', 'VISI', 'VISp', 'VISpm', 'VISrl'],\n",
    "    'Hippocampus' : ['CA1', 'CA2', 'CA3', 'DG'],\n",
    "    'SomNuc' : ['VPL', 'VPM']\n",
    "}\n",
    "# print(combined_areas.keys())\n",
    "\n",
    "# Creating a new key called 'new area' in alldat to add new_labels\n",
    "for dat in alldat:\n",
    "    new_labels = []\n",
    "    old_labels = dat['brain_area']\n",
    "    for label in old_labels:\n",
    "        found = False\n",
    "        for key in combined_areas:\n",
    "            if label in combined_areas[key]:\n",
    "                new_labels.append(key)\n",
    "                found = True\n",
    "                break\n",
    "        # if not present in the combined_areas dict\n",
    "        if not found:\n",
    "            new_labels.append(label)\n",
    "    dat['new_area'] = np.array(new_labels)\n",
    "\n",
    "sessions = [np.unique(dat['new_area']) for dat in alldat]\n",
    "chosen_ones = ['ACA', 'Hippocampus', 'MRN', 'MoCo', 'PL', 'SUB', 'VisualAreas']\n",
    "overlap  = pd.DataFrame(index=chosen_ones, columns=chosen_ones)\n",
    "for ses in sessions:\n",
    "    for i in range(len(ses)-1):\n",
    "        for j in range(i+1,len(ses)):\n",
    "            if ses[i] in chosen_ones and ses[j] in chosen_ones:\n",
    "                if overlap.loc[ses[i],ses[j]] is np.NaN:\n",
    "                    overlap.loc[ses[i],ses[j]] = 1\n",
    "                else:\n",
    "                    overlap.loc[ses[i],ses[j]] += 1\n",
    "                if overlap.loc[ses[j],ses[i]] is np.NaN:\n",
    "                    overlap.loc[ses[j],ses[i]] = 1\n",
    "                else:\n",
    "                    overlap.loc[ses[j],ses[i]] += 1\n",
    "# print(overlap)\n",
    "\n",
    "chosen_ones = ['ACA','Hippocampus','MoCo','PL', 'VisualAreas']\n",
    "sessions = [np.unique(dat['new_area']) for dat in alldat]\n",
    "overlap  = pd.DataFrame(index=chosen_ones, columns=chosen_ones)\n",
    "for ses in sessions:\n",
    "    for i in range(len(ses)-1):\n",
    "        for j in range(i+1,len(ses)):\n",
    "            if ses[i] in chosen_ones and ses[j] in chosen_ones:\n",
    "                if overlap.loc[ses[i],ses[j]] is np.NaN:\n",
    "                    overlap.loc[ses[i],ses[j]] = 1\n",
    "                else:\n",
    "                    overlap.loc[ses[i],ses[j]] += 1\n",
    "                if overlap.loc[ses[j],ses[i]] is np.NaN:\n",
    "                    overlap.loc[ses[j],ses[i]] = 1\n",
    "                else:\n",
    "                    overlap.loc[ses[j],ses[i]] += 1\n",
    "# print(overlap)\n",
    "\n",
    "# Making sure that the regions with all the chosen areas don't have other regions that are present in all\n",
    "# Uncomment the code below for sanity check \n",
    "# chosen_ones = ['ACA','Hippocampus','MoCo','PL', 'VisualAreas']\n",
    "# test = []\n",
    "# for session in alldat:\n",
    "#     session_areas = session['new_area']\n",
    "#     if (set(np.intersect1d(chosen_ones,np.unique(session_areas))) == set(chosen_ones)):\n",
    "#         test.append(session)\n",
    "# test_areas = [np.unique(x['new_area']) for x in test]\n",
    "# all_areas = np.unique(np.concatenate((test_areas)))\n",
    "# t_overlap  = pd.DataFrame(index=all_areas, columns=all_areas)\n",
    "\n",
    "# for i in range(len(test_areas)):\n",
    "#     for j in range(len(test_areas[i])-1):\n",
    "#         for k in range(i+1, len(test_areas[i])):\n",
    "#             if t_overlap.loc[test_areas[i][j],test_areas[i][k]] is np.NaN:\n",
    "#                 t_overlap.loc[test_areas[i][j],test_areas[i][k]] = 1\n",
    "#             else:\n",
    "#                 t_overlap.loc[test_areas[i][j],test_areas[i][k]] += 1\n",
    "#             if t_overlap.loc[test_areas[i][j],test_areas[i][k]] is np.NaN:\n",
    "#                 t_overlap.loc[test_areas[i][j],test_areas[i][k]] = 1\n",
    "#             else:\n",
    "#                 t_overlap.loc[test_areas[i][j],test_areas[i][k]] += 1\n",
    "\n",
    "# Next we want to restrict the dataset to the sessions which have the 5 above areas, and only the neurons which belong to these areas\n",
    "chosen_ones = ['ACA','Hippocampus','MoCo','PL', 'VisualAreas']\n",
    "copy_as_is = ['bin_size', 'stim_onset','mouse_name','date_exp','wheel','pupil','response', \\\n",
    "              'response_time','contrast_right','contrast_left','feedback_time','feedback_type',\\\n",
    "              'gocue', 'licks','face', 'reaction_time', 'ccf_axes', 'prev_reward']\n",
    "not_needed = ['waveform_w','waveform_u','brain_area_lfp','trough_to_peak','active_trials', \\\n",
    "              'contrast_left_passive','contrast_right_passive','spks_passive','lfp_passive',\\\n",
    "              'pupil_passive','wheel_passive', 'licks_passive', 'face_passive', 'cellid_orig' ]\n",
    "restricted_dat = []\n",
    "for session in alldat:\n",
    "    session_areas = session['new_area']\n",
    "    # Only if all the chosen_areas are in this session\n",
    "    res_session = {}\n",
    "    if (set(np.intersect1d(chosen_ones,np.unique(session_areas))) == set(chosen_ones)):\n",
    "        valid = []\n",
    "        for idx in range(len(session['new_area'])):\n",
    "            if session['new_area'][idx] in chosen_ones:\n",
    "                valid.append(idx)\n",
    "        for key in session:\n",
    "            if key not in not_needed:\n",
    "                if key in copy_as_is:\n",
    "                    res_session[key] = session[key]\n",
    "                else:\n",
    "                    #debug\n",
    "                    if key == 'spks':\n",
    "                        res_session[key] = np.take(session[key],valid, axis=0)\n",
    "                    else:\n",
    "                        res_session[key] = np.take(session[key],valid)\n",
    "        restricted_dat.append(res_session)\n",
    "\n",
    "# print(\"\\n Now let's look at the  number of neurons in the original areas\\n\")\n",
    "for dat in restricted_dat:\n",
    "    this_dict = {}\n",
    "    for x in np.unique(dat['brain_area']):\n",
    "        this_dict[x] = 0\n",
    "    for x in dat['brain_area']:\n",
    "        this_dict[x] += 1\n",
    "    # print(this_dict)\n",
    "\n",
    "# We decided to separate Hippocampus back into CA1 and DG since they have enough\n",
    "# neurons on their own, and the MoCO is just MOs. The Visual areas stay combined\n",
    "\n",
    "for dat in restricted_dat:\n",
    "    old_labels = dat['new_area']\n",
    "    og_labels = dat['brain_area']\n",
    "    for i in range(len(old_labels)):\n",
    "        if old_labels[i] == 'Hippocampus' or old_labels[i] =='MoCo':\n",
    "            old_labels[i] = og_labels[i]\n",
    "    dat['new_area'] = old_labels\n",
    "\n",
    "for dat in restricted_dat:\n",
    "    this_dict = {}\n",
    "    for x in np.unique(dat['new_area']):\n",
    "        this_dict[x] = 0\n",
    "    for x in dat['new_area']:\n",
    "        this_dict[x] += 1\n",
    "    # print(this_dict)\n",
    "\n",
    "# Uncomment the code below for sanity check \n",
    "# for x in restricted_dat:\n",
    "#     print(len(x['new_area']))\n",
    "# for key in restricted_dat[0]:\n",
    "#     try:\n",
    "#         print(key, ' : ', restricted_dat[0][key].shape)\n",
    "#     except:\n",
    "#         print(key, restricted_dat[0][key])\n",
    "\n",
    "# Reshape spikes from neurons x trials x timebins to trials x neurons x timebins\n",
    "\n",
    "for dat in restricted_dat:\n",
    "    dat['re_spks'] = np.swapaxes(dat['spks'], 0,1)\n",
    "\n",
    "# Uncomment the code below for sanity check    \n",
    "# for x in restricted_dat:\n",
    "#     print(len(x['new_area']))\n",
    "# for key in restricted_dat[0]:\n",
    "#     try:\n",
    "#         print(key, ' : ', restricted_dat[0][key].shape)\n",
    "#     except:\n",
    "#         print(key, restricted_dat[0][key])\n",
    "\n",
    "# for dat in restricted_dat:\n",
    "#     print(dat['gocue'].min(), dat['gocue'].max())\n",
    "\n",
    "# We decided to use a time window of 400 msec before the go-cue onset\n",
    "for dat in restricted_dat:\n",
    "    sum_spikes = []\n",
    "    for trial_num in range(len(dat['gocue'])):\n",
    "        this_gocue = dat['gocue'][trial_num]\n",
    "        # timebins = np.arange(0,2.5,0.01) #Incorrect!\n",
    "        timebins = np.arange(-0.5,2,0.01)\n",
    "        idx = np.argmin(np.abs(timebins - this_gocue))\n",
    "        spk_count = np.sum(dat['re_spks'][trial_num][:,idx-40:idx], axis = 1)\n",
    "        sum_spikes.append(spk_count)\n",
    "    # print(np.array(sum_spikes).shape)\n",
    "    dat['sum_spikes'] = np.array(sum_spikes)\n",
    "\n",
    "# Uncomment the code below for sanity check \n",
    "# for x in restricted_dat:\n",
    "#     print(len(x['new_area']))\n",
    "# for key in restricted_dat[0]:\n",
    "#     try:\n",
    "#         print(key, ' : ', restricted_dat[0][key].shape)\n",
    "#     except:\n",
    "#         print(key, restricted_dat[0][key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "70-1BcrtMIOC"
   },
   "outputs": [],
   "source": [
    "# Create regionwise data dictionary\n",
    "dat = restricted_dat[0]\n",
    "# print(dat['sum_spikes'].shape)\n",
    "regionwise_data = {}\n",
    "chosen_ones = ['ACA','CA1', 'DG', 'MOs','PL', 'VisualAreas']\n",
    "for area in chosen_ones:\n",
    "    idx = np.where(dat['new_area'] == area)[0]\n",
    "    this_area_neurons = np.take(dat['sum_spikes'],idx, axis=1)\n",
    "    regionwise_data[area] = this_area_neurons\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "nvyYYRqgWmYF",
    "outputId": "f1a59000-4ad4-4549-e30e-84d44c98d954"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your model's ( without ACA ) reported accuracy for session 0  is:  57.5 %\n",
      "Your model's ( without ACA ) reported nLL for session 0  is:  0.952\n",
      "Your model's ( without CA1 ) reported accuracy for session 0  is:  57.5 %\n",
      "Your model's ( without CA1 ) reported nLL for session 0  is:  0.942\n",
      "Your model's ( without DG ) reported accuracy for session 0  is:  57.87 %\n",
      "Your model's ( without DG ) reported nLL for session 0  is:  0.948\n",
      "Your model's ( without MOs ) reported accuracy for session 0  is:  56.31 %\n",
      "Your model's ( without MOs ) reported nLL for session 0  is:  0.953\n",
      "Your model's ( without PL ) reported accuracy for session 0  is:  58.26 %\n",
      "Your model's ( without PL ) reported nLL for session 0  is:  0.955\n",
      "Your model's ( without VisualAreas ) reported accuracy for session 0  is:  55.91 %\n",
      "Your model's ( without VisualAreas ) reported nLL for session 0  is:  0.997\n",
      "Your model's ( without ACA ) reported accuracy for session 1  is:  52.6 %\n",
      "Your model's ( without ACA ) reported nLL for session 1  is:  0.994\n",
      "Your model's ( without CA1 ) reported accuracy for session 1  is:  51.18 %\n",
      "Your model's ( without CA1 ) reported nLL for session 1  is:  0.985\n",
      "Your model's ( without DG ) reported accuracy for session 1  is:  48.24 %\n",
      "Your model's ( without DG ) reported nLL for session 1  is:  0.998\n",
      "Your model's ( without MOs ) reported accuracy for session 1  is:  53.83 %\n",
      "Your model's ( without MOs ) reported nLL for session 1  is:  1.0\n",
      "Your model's ( without PL ) reported accuracy for session 1  is:  52.96 %\n",
      "Your model's ( without PL ) reported nLL for session 1  is:  0.998\n",
      "Your model's ( without VisualAreas ) reported accuracy for session 1  is:  49.13 %\n",
      "Your model's ( without VisualAreas ) reported nLL for session 1  is:  1.024\n",
      "Your model's ( without ACA ) reported accuracy for session 2  is:  60.01 %\n",
      "Your model's ( without ACA ) reported nLL for session 2  is:  0.877\n",
      "Your model's ( without CA1 ) reported accuracy for session 2  is:  59.04 %\n",
      "Your model's ( without CA1 ) reported nLL for session 2  is:  0.923\n",
      "Your model's ( without DG ) reported accuracy for session 2  is:  55.67 %\n",
      "Your model's ( without DG ) reported nLL for session 2  is:  0.935\n",
      "Your model's ( without MOs ) reported accuracy for session 2  is:  44.34 %\n",
      "Your model's ( without MOs ) reported nLL for session 2  is:  1.043\n",
      "Your model's ( without PL ) reported accuracy for session 2  is:  60.34 %\n",
      "Your model's ( without PL ) reported nLL for session 2  is:  0.926\n",
      "Your model's ( without VisualAreas ) reported accuracy for session 2  is:  55.69 %\n",
      "Your model's ( without VisualAreas ) reported nLL for session 2  is:  0.982\n",
      "Your model's ( without ACA ) reported accuracy for session 3  is:  60.89 %\n",
      "Your model's ( without ACA ) reported nLL for session 3  is:  0.925\n",
      "Your model's ( without CA1 ) reported accuracy for session 3  is:  56.91 %\n",
      "Your model's ( without CA1 ) reported nLL for session 3  is:  0.964\n",
      "Your model's ( without DG ) reported accuracy for session 3  is:  64.02 %\n",
      "Your model's ( without DG ) reported nLL for session 3  is:  0.86\n",
      "Your model's ( without MOs ) reported accuracy for session 3  is:  53.38 %\n",
      "Your model's ( without MOs ) reported nLL for session 3  is:  0.982\n",
      "Your model's ( without PL ) reported accuracy for session 3  is:  64.82 %\n",
      "Your model's ( without PL ) reported nLL for session 3  is:  0.87\n",
      "Your model's ( without VisualAreas ) reported accuracy for session 3  is:  51.81 %\n",
      "Your model's ( without VisualAreas ) reported nLL for session 3  is:  1.014\n",
      "Your model's ( without ACA ) reported accuracy for session 4  is:  51.6 %\n",
      "Your model's ( without ACA ) reported nLL for session 4  is:  1.001\n",
      "Your model's ( without CA1 ) reported accuracy for session 4  is:  57.43 %\n",
      "Your model's ( without CA1 ) reported nLL for session 4  is:  0.943\n",
      "Your model's ( without DG ) reported accuracy for session 4  is:  56.56 %\n",
      "Your model's ( without DG ) reported nLL for session 4  is:  0.938\n",
      "Your model's ( without MOs ) reported accuracy for session 4  is:  58.89 %\n",
      "Your model's ( without MOs ) reported nLL for session 4  is:  0.942\n",
      "Your model's ( without PL ) reported accuracy for session 4  is:  51.9 %\n",
      "Your model's ( without PL ) reported nLL for session 4  is:  0.97\n",
      "Your model's ( without VisualAreas ) reported accuracy for session 4  is:  55.39 %\n",
      "Your model's ( without VisualAreas ) reported nLL for session 4  is:  0.956\n"
     ]
    }
   ],
   "source": [
    "# Create regionwise data dictionary\n",
    "sdict = {}\n",
    "\n",
    "for sid in range(len(restricted_dat)):\n",
    "    sdict[sid] = {}\n",
    "    dat = restricted_dat[sid]\n",
    "    # print(dat['sum_spikes'].shape)\n",
    "    chosen_ones = ['ACA','CA1', 'DG', 'MOs','PL', 'VisualAreas']\n",
    "    for left_out_area in chosen_ones:\n",
    "        sdict[sid][left_out_area] = {}\n",
    "        regionwise_data = {}\n",
    "        this_chosen_ones = [x for x in chosen_ones if x != left_out_area]\n",
    "#         print(this_chosen_ones)\n",
    "        for area in this_chosen_ones:\n",
    "            idx = np.where(dat['new_area'] == area)[0]\n",
    "            this_area_neurons = np.take(dat['sum_spikes'],idx, axis=1)\n",
    "            regionwise_data[area] = this_area_neurons\n",
    "\n",
    "        # Code block for using cross validation\n",
    "        # Nested cross-validation algo as mentioned in\n",
    "        # https://weina.me/nested-cross-validation\n",
    "\n",
    "        # Design Matrix\n",
    "        # Format: 1 row (1 _trial) ---> [region1_features region2_features region3_features ..]\n",
    "        # Format of region1_features: pc1 pc2 pc3 pc1*pc1 pc1*pc2 pc1*pc3 pc2*pc2 pc2*pc3 pc3*pc3\n",
    "\n",
    "        # Outer and inner loop values chosen so as to maintain 70-15-15\n",
    "        # training-valid-test proportion\n",
    "\n",
    "        trial_count = regionwise_data[this_chosen_ones[0]].shape[0]\n",
    "        all_idx = np.arange(0, trial_count)\n",
    "        kf = KFold(n_splits = 7, shuffle = True, random_state=2020)\n",
    "\n",
    "        for i in range(7):\n",
    "            sdict[sid][left_out_area][i] = {}\n",
    "            sdict[sid][left_out_area][i]['ht_dict'] = []\n",
    "            sdict[sid][left_out_area][i]['best_model'] = []\n",
    "            sdict[sid][left_out_area][i]['best_model_acc'] = []\n",
    "            sdict[sid][left_out_area][i]['best_model_NLL'] = []\n",
    "\n",
    "        reported_accuracies = []\n",
    "        reported_NLL = []\n",
    "\n",
    "        kid = 0\n",
    "        for trainval, test_idx in kf.split(all_idx):\n",
    "            X_test = {}\n",
    "            X_tv = {}\n",
    "            for key in regionwise_data:\n",
    "                X_test[key] = np.take(regionwise_data[key], test_idx, axis=0)\n",
    "                X_tv[key] = np.take(regionwise_data[key], trainval, axis=0)\n",
    "            Y_test = np.take(dat['response'], test_idx, axis = 0)\n",
    "            Y_tv = np.take(dat['response'], trainval, axis = 0)\n",
    "\n",
    "            # make dm_trainval\n",
    "            primary_features_tv = get_primary_features(X_tv)\n",
    "            dm_tv = []\n",
    "            for i in range(X_tv[this_chosen_ones[0]].shape[0]):\n",
    "                this_row = []\n",
    "                for key in X_test:\n",
    "                    this_row.extend(make_features_for_this_region(primary_features_tv[key][i]))\n",
    "                dm_tv.append(this_row)\n",
    "\n",
    "            # make dm_test\n",
    "            primary_features_test = get_primary_features(X_test)\n",
    "            dm_test = []\n",
    "            for i in range(X_test[this_chosen_ones[0]].shape[0]):\n",
    "                this_row = []\n",
    "                for key in X_test:\n",
    "                    this_row.extend(make_features_for_this_region(primary_features_test[key][i]))\n",
    "                dm_test.append(this_row)\n",
    "\n",
    "            # Normalize outer dms\n",
    "            dm_tv = normalize_dm(dm_tv)\n",
    "            dm_test = normalize_dm(dm_test)\n",
    "\n",
    "            lratio_range = np.arange(0,1.01,0.1)\n",
    "            c_range = np.logspace(-4,0, num = 5)\n",
    "            # lratio_range = np.arange(0,1.01,0.4)\n",
    "            # c_range = np.logspace(-4,4, num = 4)\n",
    "            hyper_table = pd.DataFrame(index=lratio_range, columns=c_range)\n",
    "            # initialize table with empty lists\n",
    "            for i in lratio_range:\n",
    "                for j in c_range:\n",
    "                    hyper_table.loc[i,j] = []\n",
    "\n",
    "            lf = KFold(n_splits = 5, shuffle = True, random_state=2020)\n",
    "            for train_idx, valid_idx in lf.split(trainval):\n",
    "                this_train = np.take(trainval, train_idx)\n",
    "                this_valid = np.take(trainval, valid_idx)\n",
    "                X_valid, X_train = {}, {}\n",
    "                for key in regionwise_data:\n",
    "                    X_train[key] = np.take(regionwise_data[key], this_train, axis=0)\n",
    "                    X_valid[key] = np.take(regionwise_data[key], this_valid, axis=0)\n",
    "                Y_train = np.take(dat['response'], this_train, axis = 0)\n",
    "                Y_valid = np.take(dat['response'], this_valid, axis = 0)\n",
    "\n",
    "                # make dm_valid\n",
    "                primary_features_valid = get_primary_features(X_valid)\n",
    "                dm_valid = []\n",
    "                for i in range(X_valid[this_chosen_ones[0]].shape[0]):\n",
    "                    this_row = []\n",
    "                    for key in X_valid:\n",
    "                        this_row.extend(make_features_for_this_region(primary_features_valid[key][i]))\n",
    "                    dm_valid.append(this_row)\n",
    "\n",
    "                # make dm_train\n",
    "                primary_features_train = get_primary_features(X_train)\n",
    "                dm_train = []\n",
    "                for i in range(X_train[this_chosen_ones[0]].shape[0]):\n",
    "                    this_row = []\n",
    "                    for key in X_train:\n",
    "                        this_row.extend(make_features_for_this_region(primary_features_train[key][i]))\n",
    "                    dm_train.append(this_row)\n",
    "\n",
    "                # Normalize all design matrices so that standard deviation for the column is 1\n",
    "                # This is done by dividing each column by its standard deviation\n",
    "                dm_train = normalize_dm(dm_train)\n",
    "                dm_valid = normalize_dm(dm_valid)\n",
    "\n",
    "\n",
    "                for c1 in lratio_range:\n",
    "                    for c2 in c_range:\n",
    "                        this_model = LogisticRegression(penalty='elasticnet', l1_ratio=c1, C=c2, solver='saga', fit_intercept=True, multi_class='multinomial', max_iter=10000).fit(dm_train, Y_train)\n",
    "                        this_predict = this_model.predict_proba(dm_valid)\n",
    "                        # this_correct = np.where(this_predict==Y_valid)[0].shape[0]\n",
    "                        # hyper_table.loc[c1,c2].append(this_correct/Y_valid.shape[0])\n",
    "                        this_nLL = log_loss(Y_valid, this_predict, labels=np.array([-1,0,1], dtype='float32'))\n",
    "                        hyper_table.loc[c1,c2].append(this_nLL)\n",
    "\n",
    "            # Now the inner L-fold have finished running\n",
    "            for c1 in lratio_range:\n",
    "                for c2 in c_range:\n",
    "                    hyper_table.loc[c1,c2] = np.mean(hyper_table.loc[c1,c2])\n",
    "\n",
    "            # Find the optimal value of hyper-parameters based on minimum nLL\n",
    "            this_ht = hyper_table.to_numpy(dtype ='float32')\n",
    "            c1_opt, c2_opt = np.where(this_ht == this_ht.min())\n",
    "            c1_opt, c2_opt = lratio_range[c1_opt[0]], c_range[c2_opt[0]]\n",
    "\n",
    "            sdict[sid][left_out_area][kid]['ht_dict'] = this_ht\n",
    "            # Now train a model on trainval, based on these hyper-parameters\n",
    "            this_best_model = LogisticRegression(penalty='elasticnet', l1_ratio=c1_opt, C=c2_opt, solver='saga', fit_intercept=True, multi_class='multinomial', max_iter=10000).fit(dm_tv, Y_tv)\n",
    "            this_best_prob = this_best_model.predict_proba(dm_test)\n",
    "            this_best_predict = this_best_model.predict(dm_test)\n",
    "            this_best_correct = np.where(this_best_predict==Y_test)[0].shape[0]\n",
    "            reported_accuracies.append(this_best_correct/Y_test.shape[0])\n",
    "            this_best_nLL = log_loss(Y_test, this_best_prob, labels=np.array([-1,0,1], dtype='float32'))\n",
    "            reported_NLL.append(this_best_nLL)\n",
    "\n",
    "            sdict[sid][left_out_area][kid]['best_model'] = this_best_model\n",
    "            sdict[sid][left_out_area][kid]['best_model_acc'] = this_best_correct/Y_test.shape[0]\n",
    "            sdict[sid][left_out_area][kid]['best_model_NLL'] = this_best_nLL\n",
    "            kid += 1\n",
    "\n",
    "            # Sanity check to see if it works\n",
    "            # a = hyper_table.to_numpy(dtype='float32')\n",
    "            # fig, ax = plt.subplots()\n",
    "            # pos = ax.imshow(a, cmap='jet')\n",
    "            # ax.set_yticks(np.arange(lratio_range.shape[0]))\n",
    "            # ax.set_yticklabels(np.round(lratio_range,2))\n",
    "            # ax.set_xticks(np.arange(c_range.shape[0]))\n",
    "            # ax.set_xticklabels(np.round(c_range,2))\n",
    "            # fig.colorbar(pos)\n",
    "            # break # Just to see if it works\n",
    "\n",
    "        print(\"Your model's ( without\", left_out_area, \") reported accuracy for session\", sid, \" is: \", np.round(np.mean([sdict[sid][left_out_area][x]['best_model_acc'] for x in sdict[sid][left_out_area]])*100,2), \"%\")\n",
    "        print(\"Your model's ( without\", left_out_area, \") reported nLL for session\", sid, \" is: \",  np.round(np.mean([sdict[sid][left_out_area][x]['best_model_NLL'] for x in sdict[sid][left_out_area]]),3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.959"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " np.round(np.mean([sdict[sid][x]['best_model_NLL'] for x in sdict[sid]]),3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 6  3  7]\n",
      " [ 6  4  3]\n",
      " [20  4  3]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "a = [[12,11,31],[13,13,14],[41,13,14]]\n",
    "a = np.array(a)\n",
    "# print(a)\n",
    "for i in range(a.shape[1]):\n",
    "    a[:,i] = a[:,i]/(2+i)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 282
    },
    "colab_type": "code",
    "id": "72Bk1dhtYJbc",
    "outputId": "e0732731-29c6-48c5-c69e-48504ddd0af6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.colorbar.Colorbar at 0x1c223cb9a88>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Viewing hyper-parameter dict\n",
    "a = ht_dict[0]\n",
    "fig, ax = plt.subplots()\n",
    "pos = ax.imshow(a, cmap='jet')\n",
    "ax.set_yticks(np.arange(lratio_range.shape[0]))\n",
    "ax.set_yticklabels(np.round(lratio_range,2))\n",
    "ax.set_xticks(np.arange(c_range.shape[0]))\n",
    "ax.set_xticklabels(np.round(c_range,2))\n",
    "fig.colorbar(pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 54)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample = this_best_model.coef_\n",
    "sample.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0  8]\n",
      " [ 0  9]\n",
      " [ 0 11]\n",
      " [ 0 27]\n",
      " [ 0 30]\n",
      " [ 1  0]\n",
      " [ 1  3]\n",
      " [ 1 15]\n",
      " [ 1 23]\n",
      " [ 1 25]\n",
      " [ 1 45]\n",
      " [ 2 11]\n",
      " [ 2 17]\n",
      " [ 2 40]\n",
      " [ 2 50]\n",
      " [ 2 52]]\n"
     ]
    }
   ],
   "source": [
    "print(np.argwhere(sample != 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNnD59pLGW2KIg0sqhp+0P1",
   "include_colab_link": true,
   "name": "Working_example_baseline_model.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
